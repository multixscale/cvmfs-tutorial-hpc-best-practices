{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Best Practices for CernVM-FS in HPC","text":"<p>Warning</p> <p>(Nov'23) This tutorial is under development, please come back later when the tutorial contents have been completed.</p> <p>An online version of this tutorial is planned for Mon 4 Dec 2023 (13:30-17:00 CET), register via https://event.ugent.be/registration/cvmfshpc202312.</p> <p>This is an introductory tutorial to CernVM-FS, the CernVM File System, with a focus on employing it in the context of High-Performance Computing (HPC).</p> <p>In this tutorial you will learn what CernVM-FS is, how to get access to existing CernVM-FS repositories, how to configure CernVM-FS, and how to use CernVM-FS repositories on HPC infrastructure.</p> <p>Ready to go? Click here to start the tutorial!</p>"},{"location":"#intended-audience","title":"Intended audience","text":"<p>This tutorial is intended for people with a background in HPC (system administrators, support team members, end users, etc.) and who are new to CernVM-FS; no specific prior knowledge or experience with it is required.</p> <p>We expect it to be most valuable to people who are interested in using or providing access to one or more existing CernVM-FS repositories on HPC infrastructure.</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>(more info soon)</p>"},{"location":"#practical-information","title":"Practical information","text":"<p>A first virtual edition of this tutorial is planned for Monday 4 December 2023 (13:30-17:00 CET).</p> <p>Attendance is free, but registration is required: https://event.ugent.be/registration/cvmfshpc202312.</p> <p>(more practical info soon)</p>"},{"location":"#slides","title":"Slides","text":"<p>(coming soon)</p>"},{"location":"#multixscale","title":"MultiXscale","text":"<p>This tutorial is being developed and organised in the context of the MultiXscale EuroHPC Centre-of-Excellence.</p> <p>Funded by the European Union. This work has received funding from the European High Performance Computing Joint Undertaking (JU) and countries participating in the project under grant agreement No 101093169.</p>"},{"location":"#contributors","title":"Contributors","text":"<ul> <li>Jakob Blomer (CERN, Switzerland)</li> <li>Bob Dr\u00f6ge (University of Groningen, The Netherlands)</li> <li>Kenneth Hoste (Ghent University, Belgium)</li> <li>Alan O'Cais (University of Barcelona, Spain; CECAM)</li> <li>Lara Peeters (Ghent University, Belgium)</li> <li>Laura Promberger (CERN, Switzerland)</li> <li>Thomas R\u00f6blitz (University of Bergen, Norway)</li> <li>Caspar van Leeuwen (SURF, The Netherlands)</li> <li>Valentin V\u00f6lkl (CERN, Switzerland)</li> </ul>"},{"location":"#additional-resources","title":"Additional resources","text":"<ul> <li>CernVM-FS website</li> <li>CernVM-FS documentation</li> <li>CernVM-FS @ GitHub</li> <li>Introduction to CernVM-FS by Jakob Blomer (CERN) (2021)</li> <li>Introductory tutorial on CernVM-FS (2021)</li> </ul>"},{"location":"configuration_hpc/","title":"Configuring CernVM-FS on HPC infrastructure","text":"<p>In the previous section we have outlined how to set up a robust CernVM-FS infrastructure, by having a private Stratum 1 replica server and/or dedicated Squid proxy servers. While this approach will work for many HPC systems, some may have slightly more esoteric setups that require specific solutions, which we will discuss in this section.</p>"},{"location":"configuration_hpc/#diskless-worker-nodes","title":"Diskless worker nodes","text":"<p>Some HPC systems may have worker nodes without any type of local disk, which is problematic for CernVM-FS since it uses a local cache on the worker nodes. Without this local cache, CernVM-FS can not store the repository content that is being accessed by users.</p> <p>A couple of workarounds are possible in this case:</p> <ul> <li>In-memory client cache</li> <li>Loopback filesystem on a shared filesystem</li> <li>Alien cache</li> </ul>"},{"location":"configuration_hpc/#in-memory-client-cache","title":"In-memory client cache","text":"<p>An easy way to set up a client cache on diskless systems is to use a RAM disk like <code>/dev/shm</code>.</p> <p>It suffices to use a path like <code>/dev/shm/cvmfs-cache</code> (or equivalent) as the value for the <code>CVMFS_CACHE_BASE</code> configuration setting in <code>/etc/cvmfs/default.local</code>, along with setting <code>CVMFS_QUOTA_LIMIT</code> to the amount of memory that you would like to dedicate to the CernVM-FS client cache.</p> <p>For example:</p> <pre><code># use max. 4GB of memory for CernVM-FS client cache\nCVMFS_CACHE_BASE=/dev/shm/cvmfs-cache\nCVMFS_QUOTA_LIMIT=4000\n</code></pre> <p>Do not forget to apply the changes made by running:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>An obvious significant drawback of this is that less memory will be available to workloads running on the worker nodes, but it may be worth considering especially if enough memory is available in total.</p> <p>For general information on CernVM-FS cache settings, see the CernVM-FS documentation.</p>"},{"location":"configuration_hpc/#loopback-filesystem","title":"Loopback on shared filesystem","text":"<p>The generally recommended solution for diskless worker nodes is to use a loopback filesystem for the CernVM-FS client cache, which can be stored on the cluster's shared filesystem of the HPC cluster. Every worker node will need its own file in this case.</p> <p>This ensures that the parallelism of the shared file system can be exploited, while metadata accesses are performed within the loopback filesystems, and hence not overloading the shared filesystem's metadata server(s).</p> <p>The loopback filesystem files can be created using <code>dd</code> or <code>mkfs</code>. They should be formatted as an <code>ext3</code>, <code>ext4</code>, or <code>xfs</code> file system, and should be 15% larger than the cache size configured on the nodes (with <code>CVMFS_QUOTA_LIMIT</code>).</p> <p>On the worker nodes the loopback filesystem can be mounted from the shared file system, and they should be made available at the location specified in the <code>CVMFS_CACHE_BASE</code> configuration setting (or <code>/var/lib/cvmfs</code>, by default).</p>"},{"location":"configuration_hpc/#alien-cache-diskless","title":"Alien cache","text":"<p>An alien cache is a cache that is outside of the (full) control of CernVM-FS.</p> <p>In this scenario you store the cache on a shared filesystem, and have the CernVM-FS processes on all worker nodes use and fill it simultaneously. These processes can pull in the required files that are being accessed by users/jobs, or you can even manually preload the cache.</p> <p>Using the alien cache still requires a very small local cache on the worker nodes for storing some control files. Given its size, you can store this local cache on a shared filesystem, or in memory.</p> <p>Compared to using a loopback filesystem described in the previous subsection, the drawback of storing the alien cache on your shared filesystem is that all metadata operations are now performed on the shared filesystem. Typically, this will result in a large number of metadata operations, and on many shared filesystems will be a significant bottleneck.</p> <p>For more information about an alien cache and configuring it, see the Alien Cache section in the CernVM-FS documentation.</p>"},{"location":"configuration_hpc/#offline-worker-nodes","title":"Offline worker nodes","text":"<p>Another typical scenario for HPC systems is that worker nodes do not have (direct) access to the internet.</p> <p>In the context of CernVM-FS, this means that the clients running on the worker nodes are not be able to pull in files from (external) Stratum 1 replica servers.</p> <p>For this scenario, several solutions are available as well:</p> <ul> <li>Squid proxy in local network</li> <li>Private Stratum 1 replica server</li> <li>Alien cache</li> </ul>"},{"location":"configuration_hpc/#squid-local","title":"Squid proxy in local network","text":"<p>Setting up a Squid proxy server in the internal network of the cluster, which is highly recommended regardless of whether the worker nodes are offline or not, will circumvent this issue since the worker nodes can be configured to only connect to Stratum 1 servers via the Squid proxy.</p> <p>This means that only the Squid proxy server requires internet access in order to fetch files from the Stratum 1 servers, while the clients will fetch the files from the proxy using the internal network.</p>"},{"location":"configuration_hpc/#private-stratum1","title":"Private Stratum 1 replica server","text":"<p>Similar to having a Squid proxy in the internal network of the cluster, one could also opt for setting up a private Stratum 1 replica server that is accessible by the worker nodes, or even do both.</p> <p>Again, only the private Stratum 1 server needs to connect to the internet, as it will need to regularly synchronize with a synchronisation server.</p>"},{"location":"configuration_hpc/#alien-cache-offline","title":"Alien cache","text":"<p>As a last resort, you can consider use an alien cache that is being prepopulated on a dedicated system outside of the HPC, which does have internet access.</p> <p>This alien cache can then be made available on the worker nodes, for instance by having it stored on the shared filesystem of the cluster.</p> <p>This is (again) not recommended however, for the same reason as before: this kind of setup will put significant load on the metadata server(s) of the shared filesystem.</p>"},{"location":"configuration_hpc/#worker-nodes-without-cernvm-fs","title":"Worker nodes without CernVM-FS","text":"<p>The last scenario that we cover here is for HPC systems that do not have the CernVM-FS client component installed on the worker nodes, for example because the system administrators are not willing to install, configure, and maintain a CernVM-FS installation.</p> <p>Though less ideal than a native installaton of CernVM-FS, solutions to make CernVM-FS repositories accessible even in this case exist:</p> <ul> <li>Syncing a to another filesystem</li> <li>Alternative access mechanisms</li> </ul>"},{"location":"configuration_hpc/#sync-other-filesystem","title":"Syncing a to another filesystem","text":"<p>A seemingly straightforward solution may be to synchronize (a part of) the contents of a CernVM-FS repository to another filesystem, and make that available on worker nodes.</p> <p>CernVM-FS provides the <code>cvmfs_shrinkwrap</code> utility exactly for this purpose.</p> <p>However, though the solution may sound easy, it has some severe disadvantages: <code>cvmfs_shrinkwrap</code> utility puts a heavy load on the server that is being used to pull in the contents, as it has to fetch all the contents (which may be a large amount of data) in one large bulk operation.</p> <p>In addition, the repository contents will have to be synchronized in some way, which involves rerunning this process regularly.</p> <p>Finally, this approach somewhat defeats the purpose of CernVM-FS, as you will be replacing a filesystem that is optimized for distributing software by one that most likely is not.</p>"},{"location":"configuration_hpc/#alternatives","title":"Alternative access mechanisms","text":"<p>Alternative mechanisms to access CernVM-FS repositories exist that do not require system administrator privileges, so they can be leveraged by end users of HPC infrastructure.</p> <p>Examples include using a container runtime like Apptainer, or using <code>cvmfsexec</code>.</p> <p>For more details on these alternatives, see Alternative ways to access CernVM-FS repositories.</p> <p>Parrot connector to CernVM-FS</p> <p>While Parrot is still mentioned in the CernVM-FS documentation (see here), it is no longer recommended to use it, since better alternatives (like <code>cvmfsexec</code>) are available now.</p> <p>(next: Troubleshooting for CernVM-FS repositories)</p>"},{"location":"containers/","title":"Containers and CernVM-FS","text":"<p>CernVM-FS can also be used to distribute container images, providing many of the same benefits that come with any CernVM-FS installation. Especially the on-demand download of accessed files means that containers start nearly instantly, and are more efficient for large images when only a fraction of the files are read, which is typically the case.</p> <p>Any CernVM-FS repository can be used to distribute container images (although often, dedicated repositories are used, like <code>/cvmfs/unpacked.cern.ch</code>).</p> <p>In order to provide de-duplication and on-demand download, images must be stored unpacked. This requires some dedicated tools, provided by CernVM-FS itself - see the section \"Ingesting container images\" below.</p>"},{"location":"containers/#accessing-via-apptainer","title":"Accessing via Apptainer","text":"<p>Apptainer is the recommended way to run containers from CernVM-FS, as it can start a container directly from an unpacked root file system, which is ideal for CernVM-FS.</p> <p>Docker can be used as well but the setup is more complicated, requiring the CernVM-FS graphdriver plugin.</p> <p>For example, to run the <code>tensorflow/tensorflow:2.15.0-jupyter</code> image from Docker Hub that has been unpacked in <code>/cvmfs/unpacked.cern.ch</code>, use the following commands:</p> <pre><code>container=\"registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter\"\npython_code=\"import tensorflow as tf; print(tf.__version__)\"\napptainer run /cvmfs/unpacked.cern.ch/${container} python -c \"${python_code}\"\n</code></pre> <p>This directory just contains the root file system of the image:</p> <pre><code>ls /cvmfs/unpacked.cern.ch/registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter\n</code></pre>"},{"location":"containers/#ingesting","title":"Ingesting container images","text":"<p>CernVM-FS provides a suite of container unpacking tools called <code>cvmfs_ducc</code> (provided by the <code>cvmfs-ducc</code> package). This can be used to unpack and ingest container images by simply running</p> <p><pre><code>cvmfs_ducc convert recipe.yaml \n</code></pre> where <code>recipe.yaml</code> is a 'wishlist' of container images available in external registries that should be made available:</p> <pre><code>version: 1\nuser: cvmfsunpacker\ncvmfs_repo: 'unpacked.repo.tld'\ninput:\n    - 'https://registry.hub.docker.com/tensorflow/tensorflow:2.15.0-jupyter'\n    ...\n</code></pre> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"containers/#using-cvmfs-inside-containers","title":"Using <code>/cvmfs</code> inside containers","text":"<p>The easiest way to access CernVM-FS repositories from a container is to set it up on the host and bind-mount it inside the container:</p> <pre><code>docker run -it --volume /cvmfs:/cvmfs:shared ubuntu ls -lna /cvmfs/atlas.cern.ch\n</code></pre> <p>For Apptainer, the same can be done by setting the <code>$SINGULARITY_BIND</code> (or <code>$APPTAINER_BIND</code>) environment variable: </p> <pre><code>export SINGULARITY_BIND=\"/cvmfs\"\n</code></pre>"},{"location":"getting-started/","title":"Getting started with CernVM-FS (from scratch)","text":""},{"location":"getting-started/#setting-up-the-stratum-0-server","title":"Setting up the Stratum-0 server","text":""},{"location":"getting-started/#creating-a-cernvm-fs-repository","title":"Creating a CernVM-FS repository","text":""},{"location":"getting-started/#setting-up-a-stratum-1-replica-server","title":"Setting up a Stratum-1 replica server","text":""},{"location":"performance/","title":"Performance aspects of CernVM-FS","text":"<p>One aspect we can not ignore in the context of software on HPC infrastructure is performance (the P in HPC).</p>"},{"location":"performance/#start-up-performance","title":"Start-up performance","text":"<p>When installations of scientific software applications are provided via a CernVM-FS repository, the main performance metric to worry about is start-up time: the amount of time it takes until an application can start running. This requires that not only the binary program that is being launched itself is somehow available on the system (on disk, in memory, etc.), but also that all files required by it (libraries, dependencies) are available, including the ones they require in turn, etc.</p> <p>Parallel filesystems like GPFS (now IBM Storage Scale) and Lustre, which are ubiquitous on HPC systems, are notorious for not performing well in this respect, which is not surprising since they mainly target a very different use case: large-scale high-performance I/O on large datasets.</p> <p>This has led to to all sorts of creative workarounds for \"the startup problem\", including for example tools like Spindle, and recommendations to not install (in particular Python) software directly on the parallel filesystem but to use a container image instead (see the documentation of the current flagship EuroHPC system LUMI).</p> <p>A note on the presented performance results</p> <p>The start-up timing results shown in this section are by no means meant to be a statistically rigorous study of software start-up time.</p> <p>That said, these results should be indicative of what you may see on production HPC systems, and give a good view on the start-up times that you will observe for software provided via CernVM-FS, relative to alternatives like GPFS, Lustre, NFS, local disk, etc.</p> <p>More details on the experimental setup are available at the end of this section.</p>"},{"location":"performance/#status-of-relevant-caches","title":"Status of relevant caches","text":"<p>The status of the Linux kernel file system cache, the GPFS caching mechanisms (page pool, stat cache, etc.), and the CernVM-FS client cache are key factors in start-up performance (when they are relevant to the file system being accessed).</p> <p>We discriminate between 3 scenarios: cold cache, hot cache, and warm cache.</p>"},{"location":"performance/#cold_cache","title":"Cold cache","text":"<p>We use the term \"cold cache\" when none of files required for starting a particular software application are available in any of the relevant client-side caches.</p> <p>This is the worst-case scenario, since the necessary files have to be obtained elsewhere first, which could be from local disk, or from a remote server (either in the local network, or remotely), which may heavily impact start-up time.</p> <p>To ensure that the caches on the client system are cold, we actively clear all caches each time before we evaluate each start-up performance.</p>"},{"location":"performance/#hot_cache","title":"Hot cache","text":"<p>When all files required for running a particular software application are already cached in memory on the client system (in the kernel file system cache, or in the GPFS caches), we have a \"hot cache\" situation.</p> <p>This is the best possible scenario, which is typically the case when the software application has been used recently on that client system.</p> <p>To ensure that the caches on the client system are hot, we first do a couple of warmup runs before we start evaluating start-up performance.</p>"},{"location":"performance/#warm_cache","title":"Warm cache","text":"<p>Finally, we also consider a \"warm cache\" situation, in particular when evaluating start-up time for software provided via CernVM-FS. In this situation, all necessary files are available in the CernVM-FS client cache (which is typically on the local disk of the client), but not yet in the in-memory kernel file system cache. As a result, there will be an impact on start-up performance, since the files have to be loaded from (local) disk first.</p> <p>This is a fairly likely scenario, especially if the CernVM-FS client cache if sufficiently large to cover typical workloads being run \u2013 whether that's feasible or not depends a lot on the workload mix.</p> <p>To evaluate start-up performance in this scenario, we:</p> <ol> <li>First do a couple of warmup runs to populate the CernVM-FS client cache;</li> <li>Clear the kernel file system cache;</li> <li>Evaluate the start-up performance once;</li> </ol> <p>Steps 2 and 3 are then repeated to get multiple performance results in the same scenario.</p>"},{"location":"performance/#real-world-scenario","title":"Real-world scenario","text":"<p>In a production setup of an HPC cluster, where typically multiple thousands of jobs are being run every day, the actual situation will usually be a mix of these 3 idealistic scenarios, since some files may have been accessed very recently (for example common libraries from the EESSI compatibility layer, or common dependencies from the EESSI software layer), while others may only be in the CernVM-FS client cache, or not cached yet at all (or may have been evicted from the cache due to space constraints).</p>"},{"location":"performance/#tensorflow-start-up-performance","title":"TensorFlow start-up performance","text":"<p>We have evaluated the start-up performance of TensorFlow across a wide range of system configurations, by timing how long it takes to run the following command:</p> <pre><code>python -c 'import tensorflow'\n</code></pre> <p>Although this is a trivial single-core workload, it yields some interesting insights into start-up performance for software provided via CernVM-FS and alternatives.</p> <p>More details on this workload, in particular an overview of how many files are required to run it, are available in the Test workload details section.</p>"},{"location":"performance/#local-filesystems","title":"Local filesystems","text":"<p>To set the stage, we first evaluate start-up performance using only local filesystems on the client: local disk (SSD, <code>ext4</code>, <code>/tmp</code>) and RAM disk (DDR4 RAM memory, <code>/dev/shm</code>).</p> <p>Using RAM disk for all software installations is not exactly a realistic scenario on an HPC cluster, because of the wide variety of software applications and libraries that are typically employed, and the total amount of storage space that is required to host the necessary software installations. Nevertheless, it is worth evaluating start-up performance with a setup like this, if only to have a clear view on what the best possible start-up time is that we can achieve.</p> <p>Using local disk to provide a large central software stack that caters to the needs of the diverse set of end users is unlikely to be feasible either. In addition, this approach implies a significant maintenance burden since the provided software installations must be identical across all client systems at all times, and typically requires limiting both what is centrally provided and how long software versions remain available. There are HPC sites that do take this approach however, one notable example being TACC, who provide an overview of their central software stack here.</p> <p> </p> <p>These first results show that starting TensorFlow takes ~2.0 seconds when the necessary files can be served directly from RAM disk, and ~2.1 seconds from the kernel file system cache (hot cache).</p> <p>With a cold cache and using software installations located on a local SSD disk, the start-up time of TensorFlow roughly doubles to ~4.3 seconds.</p> <p>These timings will be our reference point going forward.</p>"},{"location":"performance/#parallel-file-systems","title":"Parallel file systems","text":"<p>Central software stacks on HPC systems are typically provided via some kind of distributed parallel file system, since that significantly reduces the maintenance burden (automatic synchronisation across client systems), and effectively removes the size limitation aspect since these parallel file systems typically have tens to thousands (or more) of TBs of available disk space.</p>"},{"location":"performance/#nfs","title":"NFS","text":"<p>Let's start with a traditional and fairly standard approach, which is to make a central software stack available via an export of a centrally managed shared file system using NFS. In this particular case we evaluated TensorFlow start-up time on a worker node of the HPC-UGent Tier-2 cluster victini, which uses NFS-Ganesha.</p> <p> </p> <p>Here we already observe a significant, yet still somewhat limited, increase in start-up time.</p> <p>With a cold cache, ~6.4 seconds are required to start TensorFlow, and there is notably larger variation in the timings; neither of these observations is a surprise with NFS.</p> <p>The start-up time with a hot cache is also significantly higher compared to using a local filesystems at ~3.8 seconds, but since this is an older generation system we can not directly compare these timings to the ones observed for local disk and RAM disk earlier. There is a significant penalty involved though when using NFS, since on the same system we observed start-up times of ~2.5 seconds when using RAM disk, as well as with a hot kernel file system cache.</p> <p>While these timings are still quite OK, it worth noting that NFS is notorious for not scaling well when it needs to serve many different client systems, which is not apparent in these results since only a single client system was used.</p>"},{"location":"performance/#lustre","title":"Lustre","text":"<p>Lustre is a popular open-source distributed parallel file system that is used on many large-scale HPC clusters, including VSC Tier-1 Hortense and flagship EuroHPC pre-exascale system LUMI.</p> <p>It is often also the only shared file system that is available on these systems, and hence is the most obvious choice to make software installations available to the cluster worker nodes.</p> <p> </p> <p>We considered two different ways of accessing the Lustre file system of VSC Tier-1 Hortense (see below for more setails) on which TensorFlow was installed:</p> <ul> <li>directly, via a read-write mount point;</li> <li>indirectly, via a read-only mount point (which is recommended on Hortense);</li> </ul> <p>In both cases, the start-up timings were quite similar, with a dramatic increase to ~75 seconds in the cold cache situation, and a very reasonable ~2.8-2.9 seconds with a hot cache.</p> <p>The startup-time of well over 1 minute for TensorFlow with a cold cache is a clear sign that (this particular version and configuration of) Lustre is not well suited for serving software installations. This is a well-known problem, see for example the \"Avoid Accessing Executables on Lustre Filesystems\" recommendation that is included in the Lustre Best Practices documentation published by the HECC group at NASA for their Pleiades system, and the recommendation for LUMI to not install Python software directly on the Lustre file system.</p>"},{"location":"performance/#gpfs","title":"GPFS","text":"<p>Another very popular (commercial) distributed parallel file system is IBM Storage Scale, formerly known as IBM Spectrum Scale, and commonly referred to by its original name GPFS.</p> <p>We evaluated the start-up performance of TensorFlow on the 4 different GPFS file systems that are available in the HPC-UGent Tier-2 infrastructure (see here for more information):</p> <ul> <li><code>apps</code>, which provides the central software stack (along with home directories);</li> <li><code>data</code>, which is intended for long-term storage of large volumes of data;</li> <li><code>scratch</code>, which is recommended for live input/output of jobs;</li> <li><code>scratch (SSD)</code>, an additional smaller scratch file system backed by SSDs, which is recommended for I/O-intensive workloads;</li> </ul> <p> </p> <p>Cold cache start-up times for TensorFlow ranging from ~35 to ~48 seconds are observed for the <code>apps</code>, <code>data</code>, and <code>scratch</code> file systems, with only the <code>scratch (SSD)</code> file system exhibiting somewhat reasonable cold start-up times of ~5.9 seconds.</p> <p>While there is a clear decrease in start-up times when the various GPFS caching mechanisms (page pool, stat cache, file cache) are hot, the situation is still quite dramatic with ~27 to ~33 seconds being needed to start TensorFlow, except for <code>scratch (SSD)</code> where it takes ~5.1 seconds. This implies that the GPFS caching mechanism is not that effective, and that we are still being exposed a lot to the file system latency.</p> <p>The main reason for this is that the GPFS \"stat cache\", which is a client-side caching mechanism for <code>stat</code> system calls that is common across all GPFS files systems available on that client, is set to only have 1,000 entries in the HPC-UGent Tier-2 setup (which is the default for the <code>maxStatCache</code> configuration setting in GPFS). While this could obviously be tuned to improve start-up performance of software, this should not be done on a hunch since it may adversely affect the overall stability of the GPFS storage servers.</p>"},{"location":"performance/#cernvm-fs","title":"CernVM-FS","text":"<p>Now that we have an extensive set of reference timings, we can evaluate the start-up performance when using a TensorFlow installation that is provided via CernVM-FS.</p> <p>Along with a worker node in the HPC-UGent Tier-2 cluster doduo (located in Ghent, Belgium) as client system, we consider the following configurations:</p> <ul> <li>a private proxy server that is accessible from the client system via a high-speed network;</li> <li>a private Stratum 1 server that is available in the local network via Gbit   Ethernet;</li> <li>the public Stratum 1 server for EESSI in AWS availability zone <code>eu-central-1a</code>;</li> <li>the public Stratum 1 server for EESSI in Azure location <code>East US (Zone 1)</code>;</li> </ul> <p>More technical details on network connectivity to these servers are available below.</p> <p>CernVM-FS was explicitly configured to only use on of these servers, via the <code>CVMFS_SERVER_URL</code> and/or <code>CVMFS_HTTP_PROXY</code> client configuration settings.</p>"},{"location":"performance/#cold-cache","title":"Cold cache","text":"<p>Let's start with the worst-case cold cache scenario, in which CernVM-FS needs to download all necessary files to the client system from a (local or remote) server before TensorFlow can be started.</p> <p> </p> <p>We observe very reasonable cold start-up times when using an in-network private proxy server (~8.6 seconds) or private Stratum 1 replica server (~11.0 seconds), significantly better than what we observed for GPFS and Lustre (both of which are accessible from cluster worker nodes via a dedicated high-speed network).</p> <p>When CernVM-FS needs to download the necessary files from a remote public Stratum 1 server we of course observe dramatically longer start-up times, from ~62 seconds when using the relatively close one (within Europe), to about 6 minutes when having to go across the Atlantic.</p> <p>This highlights the need for a private proxy or Stratum 1 replica server for a production-quality CernVM-FS setup.</p>"},{"location":"performance/#hot-warm-cache","title":"Hot + warm cache","text":"<p>In the hot cache and warm cache scenarios, in which all necessary files are already available locally on the client system either in-memory or on the local disk, we observe much better start-up times, as expected:</p> <p> </p> <p>With a hot kernel file system cache, we see an average start-up time for TensorFlow of ~2.25 seconds, which suggests there is a ~7% overhead introduced by CernVM-FS. Note that this is still significantly better than what we observed for NFS, Lustre, and GPFS.</p> <p>In the warm cache scenarios where CernVM-FS serves all required files from its client cache on a local SSD disk we see start-up times that are on average ~3.9-4.3 seconds, depending on the network latency to the remote server used in the CernVM-FS client configuration (and interestingly, with a larger variance when using a local proxy or Stratum 1, which may be because of other activity on the local cluster network). We did not try to optimize these warm start-up times by changing the CernVM-FS configuration settings, but some options are already supported by CernVM-FS that could reduce the impact of network latency considerably.</p> <p>When using a RAM disk as location for the CernVM-FS client cache in combination with a private proxy server, the start-up time in the warm cache scenario was reduced to ~2.56 seconds, and a lot less variation was observed (although that could be coincidental).</p>"},{"location":"performance/#additional-performance-metrics","title":"Additional performance metrics","text":"<p>Next to software start-up time on a single client system, additional performance metrics should be evaluated as well, including:</p> <ul> <li>The scalability of a CernVM-FS setup when lots of worker nodes of an HPC cluster use software   provided via CernVM-FS, in particular when running (large-scale) MPI applications;</li> <li>The impact of having the CernVM-FS \"service\" running on the client systems, in particular to what extent   it increases OS jitter which can impact performance of workloads;</li> </ul> <p>A more extensive performance study that covers these aspects as well is out of scope for this tutorial.</p>"},{"location":"performance/#test-configuration-details","title":"Test configuration details","text":"<p>A multitude of different system configurations is considered to evaluate start-up performance of the test workloads.</p>"},{"location":"performance/#client-system","title":"Client system","text":"<p>The client system used in the tests is a worker node of the HPC-UGent Tier-2 cluster \"doduo\", with two exceptions:</p> <ul> <li>When testing NFS, the HPC-UGent Tier-2 cluster \"victini\" was used instead;</li> <li>When testing Lustre, the VSC Tier-1 cluster \"Hortense\" was used instead;</li> </ul> <p>(see System configurations below for more technical details)</p>"},{"location":"performance/#software-stack","title":"Software stack","text":"<p>Software installations being used are available via either:</p> <ul> <li>a GPFS filesystem, directly attached to the cluster via a high-speed network;</li> <li>a Lustre filesystem, directly attached to the cluster via a high-speed network;</li> <li>an NFS mount of a GPFS filesystem, via a 10Gbit Ethernet connection;</li> <li>CernVM-FS, with:<ul> <li>with a client cache on local disk (SSD, <code>ext4</code>), or in RAM disk (<code>/dev/shm</code>);</li> <li>without and with (only) a (private) proxy server in the local network;</li> <li>without and with (only) a private Stratum 1 replica server in the network;</li> <li>with (only) a specific public Stratum 1 replica server: one in AWS <code>eu-west</code> region,   another in Azure <code>us-east</code> region;</li> </ul> </li> </ul> <p>(see System configurations below for more technical details)</p>"},{"location":"performance/#test_workload_details","title":"Test workload details","text":""},{"location":"performance/#tensorflow_details","title":"TensorFlow","text":"<p>We evaluate the start-up performance of TensorFlow, which is considered to be a representative example of a scientific workload implemented in Python, which is a tremendously popular programming language in scientific research. We used TensorFlow version 2.13.0, installed with EasyBuild v4.8.2, on top of Python version 3.11.3, which is available in EESSI.</p> <p>Before starting TensorFlow we first load the module to update the environment such that TensorFlow is available:</p> <pre><code>module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>To evaluate the start-up performance of TensorFlow, we simply run:</p> <pre><code>python -c 'import tensorflow'\n</code></pre> <p>Timing information is collected using the GNU <code>time</code> command, as follows:</p> <pre><code>/usr/bin/time --format '%e' python -c 'import tensorflow'\n</code></pre>"},{"location":"performance/#required-files","title":"Required files","text":"<p>Based on the statistics for and contents of the CernVM-FS client cache after running the specified command on an <code>x86_64</code> client system (see the details on the HPC-UGent Tier-2 cluster doduo below), starting from a cold CernVM-FS client cache, we know that importing the <code>tensorflow</code> Python package:</p> <ul> <li>triggers ~3,680 <code>open()</code> calls + ~510 <code>opendir</code> calls (which includes non-existing paths);</li> <li>requires ~3,470 files, including:<ul> <li>~2,200 files from the TensorFlow installation itself (~94% <code>*.pyc</code> files);</li> <li>~950 files from Python packages outside of the TensorFlow installation directory,    of which are ~82% <code>*.pyc</code> files and ~12% shared libraries (<code>.so</code>);</li> <li>17 files from the EESSI compatibility layer;</li> </ul> </li> <li>pulls in about ~1.1GB of data in total;</li> </ul> <p>As such, this is a challenge for parallel filesystems like GPFS and Lustre, as the performance results clearly show.</p>"},{"location":"performance/#experimental_setup","title":"Experimental setup","text":""},{"location":"performance/#system_configurations","title":"System configurations","text":"HPC-UGent Tier-2 cluster 'doduo' <p>Hardware:</p> <ul> <li>Dual-socket AMD EPYC 7552 CPU (AMD Rome, 96 cores in total)</li> <li>256GB of DDR4 RAM memory</li> <li>240GB SSD local disk (<code>ext4</code>)</li> <li>HDR-100 InfiniBand interconnect</li> </ul> <p>Operating system:</p> <ul> <li>Red Hat Enterprise Linux 8.8</li> <li>Linux kernel <code>4.18.0-477.27.1.el8_8.x86_64</code></li> <li>GPFS (now IBM Storage Scale) version 5.1.8-2 (<code>pagepool=4G</code>, <code>maxStatCache=1000</code>, <code>maxFilesToCache=4000</code>)</li> <li>CermVM-FS 2.11.2</li> </ul> <p>(see also HPC-UGent Tier-2 infrastructure overview)</p> HPC-UGent Tier-2 cluster 'victini' <p>Hardware:</p> <ul> <li>Dual-socket AMD EPYC 7552 CPU (AMD Rome, 96 cores in total)</li> <li>256GB of DDR4 RAM memory</li> <li>240GB SSD local disk (<code>ext4</code>)</li> <li>10Gbit Ethernet interconnect</li> </ul> <p>Operating system:</p> <ul> <li>Red Hat Enterprise Linux 8.8</li> <li>Linux kernel <code>4.18.0-477.27.1.el8_8.x86_64</code></li> <li>NFS-Ganesha 3.5</li> <li>CermVM-FS 2.11.2</li> </ul> <p>(see also HPC-UGent Tier-2 infrastructure overview)</p> VSC Tier-1 cluster 'Hortense' <p>Hardware:</p> <ul> <li>Dual-socket Intel Xeon Gold 6140 (CPU Skylake, 36 cores in total)</li> <li>96GB of DDR4 RAM memory</li> <li>900GB SAS HDD local disk (<code>ext4</code>)</li> <li>HDR-100 InfiniBand interconnect</li> </ul> <p>Operating system:</p> <ul> <li>Red Hat Enterprise Linux 8.8</li> <li>Linux kernel <code>4.18.0-477.27.1.el8_8.x86_64</code></li> <li>Lustre 2.12.9</li> <li>CermVM-FS 2.11.2 (with client cache of 4GB on SSD local disk)</li> </ul> <p>(see also VSC documentation page on Hortense)</p> Network details <p>Bandwidth</p> <p>Network bandwidth to HPC-UGent Tier-2 <code>doduo</code> cluster worker node from relevant servers, as measured with <code>iperf3</code> v3.15:</p> <ul> <li>private Squid proxy server in HPC-UGent network: ~22,500 Mbits/sec</li> <li>private Stratum 1 replica server in HPC-UGent network: ~940 Mbits/sec</li> <li>EESSI Stratum 1 replica server in AWS <code>eu-west</code> region: ~930 Mbits/sec</li> <li>EESSI Stratum 1 replica server in Azure <code>us-east</code> region: ~280 Mbits/sec</li> </ul> <p>Server-side <code>iperf3</code> command: <code>iperf3 -V -s -p 80</code> Client-side <code>iperf3</code> command: <code>iperf3 -V -c SERVER_HOSTNAME_OR_IP -p 80 -f m</code></p> <p>Latency</p> <p>Network latency between HPC-UGent Tier-2 <code>doduo</code> cluster worker node and relevant servers, as measured with <code>tcptraceroute</code> v2.1.0-6:</p> <ul> <li>private Squid proxy server in HPC-UGent network: ~0.2 ms</li> <li>private Stratum 1 replica server in HPC-UGent network: ~0.7 ms</li> <li>EESSI Stratum 1 replica server in AWS <code>eu-west</code> region: ~14 ms</li> <li>EESSI Stratum 1 replica server in Azure <code>us-east</code> region: ~84 ms</li> </ul>"},{"location":"performance/#relevant-commands","title":"Relevant commands","text":"Kernel file system cache <p>To clear kernel file system cache:</p> <pre><code>sudo sysctl -w vm.drop_caches=3\n</code></pre> <p>To check file system cache usage:</p> <pre><code>vmstat -s -S M | grep buffer\n</code></pre> CernVM-FS client cache <p>To clear the CernVM-FS client cache:</p> <pre><code>sudo cvmfs_config wipecache\n</code></pre> <p>To check size of CernVM-FS client cache on disk (path determined by <code>CVMFS_CACHE_BASE</code> configuration setting):</p> <pre><code>sudo du -sh /var/lib/cvmfs\n</code></pre> <p>To check CernVM-FS client cache usage:</p> <pre><code>cvmfs_config stat -v\n</code></pre> <p>To check CernVM-FS client cache usage for a particular repository:</p> <pre><code>cvmfs_config stat -v software.eessi.io\n</code></pre> <p>To check which files are included in the CernVM client cache, use:</p> <pre><code>sudo cvmfs_talk -i software.eessi.io cache list\n</code></pre> GPFS caching mechanisms <p>To clear all caches used by GPFS (page pool, stat cache, files cache, etc.), we simply shut down GPFS on the client system, and restart it:</p> <pre><code>mmshutdown\nmmstartup\n</code></pre> <p>This should only be done on an idle system, since this will trigger an unmount of all GPFS file systems on that client.</p> <p>(next: Containers and CernVM-FS)</p>"},{"location":"troubleshooting/","title":"Troubleshooting for CernVM-FS","text":"<p>When you experience problems with getting access to a CernVM-FS repository, it can be tricky to figure out what the actual underlying cause is, given the complexity of a typical CernVM-FS setup.</p> <p>In this section we provide some guidelines on how to troubleshoot some potential problems you may run into with CernVM-FS. We focus on client-side issues: problems that may arise on a CernVM-FS client system itself, or with the connection to external servers like a Squid proxy or a Stratum 1 replica server.</p> <p>While some commands suggested below that can help to determine the actual problem at hand require system administrator privileges (indicated with the use of <code>sudo</code>), several can also be run as an unprivileged user.</p> <p> </p> <p>Note</p> <p>In this section, we will continue to use the EESSI CernVM-FS repository <code>software.eessi.io</code> as a running example, but the troubleshooting guidelines are by no means specific to EESSI.</p> <p>Make sure you adjust the example commands to the CernVM-FS repository you are using, if needed.</p>"},{"location":"troubleshooting/#typical-problems","title":"Typical problems","text":""},{"location":"troubleshooting/#error-messages","title":"Error messages","text":"<p>The error messages that you may encounter when accessing a CernVM-FS repository are often quite cryptic, especially if you are not very familiar with CernVM-FS, or with file systems and networking on Linux systems in general.</p> <p>Here are a couple of examples:</p> <ul> <li> <p>The CernVM-FS repository may not be known (yet) on your system, which will result   in a (clear) error message like this when you try to access it:   <pre><code>$ ls /cvmfs/software.eessi.io\nls: cannot access '/cvmfs/software.eessi.io': No such file or directory\n</code></pre></p> </li> <li> <p>You may see errors messages that suggest network connectivity problems, like:   <pre><code>Failed to discover HTTP proxy servers (23 - proxy auto-discovery failed)\n</code></pre> <pre><code>\n</code></pre>   We will outline some approaches that should help you to determine what could be wrong exactly.</p> </li> <li> <p>Other problems may be quite specific to the internals of CernVM-FS,   rather than being configuration or networking issues. Examples include:   <pre><code>Failed to initialize root file catalog (16 - file catalog failure)\n</code></pre> <pre><code>Failed to transfer ownership of /var/lib/cvmfs/shared to cvmfs\n</code></pre> <pre><code># indicates that FUSE has crashed\nTransport endpoint is not connected\n</code></pre> <pre><code>ls: cannot open directory '/cvmfs/config-repo.cern.ch': Too many levels of symbolic links\n</code></pre>   Also here we will give some advice on how you might figure out what is wrong.</p> </li> </ul>"},{"location":"troubleshooting/#lag-andor-hangs","title":"Lag and/or hangs","text":"<p>When you notice lag of even (perceived) hanging when accessing a CernVM-FS repository, you should consider revising the connectivity- and cache-related configuration settings.</p>"},{"location":"troubleshooting/#general-approach","title":"General approach","text":"<p>In general, it is recommended to take a step-by-step approach to troubleshooting:</p> <ul> <li>Start with verifying the CernVM-FS (client) installation;</li> <li>Review the CernVM-FS configuration;</li> <li>Consider potential network connectivity issues;</li> <li>Keep an eye out for mounting problems;</li> <li>Make sure you have sufficient available resources like memory and local disk space;</li> <li>Rule out any cache-related shenanigans;</li> </ul> <p>Always keep in mind to check the logs, and employ the general tools that we put forward.</p>"},{"location":"troubleshooting/#common-problems","title":"Common problems","text":""},{"location":"troubleshooting/#installation","title":"CernVM-FS installation","text":"<p>Make sure that CernVM-FS is actually installed (correctly).</p> <p>Check whether both the <code>/cvmfs</code> directory and the <code>cvmfs</code> service account exists on the system: <pre><code>ls /cvmfs\nid cvmfs\n</code></pre></p> <p>Either of these errors would be a clear indication that CernVM-FS is not installed, or that the installation was not completed: <pre><code>ls: cannot access '/cvmfs': No such file or directory\n</code></pre> <pre><code>id: \u2018cvmfs\u2019: no such user\n</code></pre></p> <p>You can also check whether the <code>cvmfs2</code> command is available, and working:</p> <pre><code>cvmfs2 --help\n</code></pre> <p>which should produce output that starts with:</p> <pre><code>The CernVM File System\nVersion 2.11.2\n</code></pre> <p>In case of problems, revise the section on installing the CernVM-FS client.</p>"},{"location":"troubleshooting/#configuration","title":"CernVM-FS configuration","text":"<p>A common issue is incorrectly configuring CernVM-FS, either by making a silly mistake in a configuration file, or by not taking into account the hierarchy of configuration files that CernVM-FS considers.</p>"},{"location":"troubleshooting/#reloading","title":"Reloading","text":"<p>Don't forget to reload the CernVM-FS configuration after you've made changes to it:</p> <pre><code>sudo cvmfs_config reload\n</code></pre>"},{"location":"troubleshooting/#show-configuration","title":"Show configuration","text":"<p>Verify the configuration via <code>cvmfs_config showconfig</code>:</p> <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre> <p>We strongly advise combining this command with <code>grep</code> to check for specific configuration settings, like:</p> <pre><code>$ cvmfs_config showconfig software.eessi.io | grep CVMFS_SERVER_URL\nCVMFS_SERVER_URL='http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io;http://azure-us-east-s1.eessi.science/cvmfs/software.eessi.io'    # from /cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf\n</code></pre> <p>Be aware that <code>cvmfs_config showconfig</code> will read the configuration files as they are currently, but that does not necessarily mean that those configuration settings are currently active; see also reloading.</p>"},{"location":"troubleshooting/#non-existing-repositories","title":"Non-existing repositories","text":"<p>Keep in mind that <code>cvmfs_config</code> does not check whether the specified repository is actually known at all. Try for example querying the configuration for the fictional <code>vim.or.emacs.io</code> repository:</p> <pre><code>cvmfs_config showconfig vim.or.emacs.io\n</code></pre>"},{"location":"troubleshooting/#active_configuration","title":"Inspect active configuration","text":"<p>Inspect the active configuration that is currently used by talking to the running CernVM-FS service via <code>cvmfs_talk</code>.</p> <p>Note</p> <p>This requires that the specified CernVM-FS repository is currently mounted.</p> <pre><code>ls /cvmfs/software.eessi.io &gt; /dev/null  # to trigger mount if not mounted yet\nsudo cvmfs_talk -i software.eessi.io parameters\n</code></pre> <p><code>cvmfs_talk</code> can also be used to query other live aspects of a particular repository, see the output of <code>cvmfs_talk --help</code>. For example:</p> <ul> <li>The current revision of repository contents (via <code>revision</code>);</li> <li>Information on the Stratum 1 replica server being used (via <code>host ...</code>);</li> <li>Information on the proxy server being used (via <code>proxy ...</code>);</li> <li>Information on the CernVM-FS client cache (via <code>cache ...</code>);</li> </ul>"},{"location":"troubleshooting/#non-mounted-repositories","title":"Non-mounted repositories","text":"<p>If running <code>cvmfs_talk</code> fails with an error like \"<code>Seems like CernVM-FS is not running</code>\", try triggering a mount of the repository first by accessing it (with <code>ls</code>), or by running:</p> <pre><code>cvmfs_config probe software.eessi.io\n</code></pre> <p>If the latter succeeds but accessing the repository does not, there may be an issue with the (active) configuration, or there may be a connectivity problem.</p>"},{"location":"troubleshooting/#connectivity","title":"Connectivity issues","text":"<p>There could be various issues related to network connectivity, for example a firewall blocking connections.</p> <p>CernVM-FS uses plain <code>HTTP</code> as data transfer protocol, so basic tools can be used to investigate connectivity issues.</p> <p>You should check whether the client system can connect to the Squid proxy and/or Stratum-1 replica server(s).</p>"},{"location":"troubleshooting/#determine_proxy","title":"Determine proxy server","text":"<p>First figure out if a proxy server is being used via: <pre><code>sudo cvmfs_talk -i software.eessi.io proxy info\n</code></pre></p> <p>This should produce output that looks like:</p> <pre><code>Load-balance groups:\n[0] http://PROXY_IP:3128 (PROXY_IP, +6h)\n[1] DIRECT\nActive proxy: [0] http://PROXY_IP:3128\n</code></pre> <p>(to protect the innocent, the actual proxy IP was replaced with \"<code>PROXY_IP</code>\" in the output above)</p> <p>The last line indicates that a proxy server is indeed being used currently.</p> <p><code>DIRECT</code> would mean that no proxy server is being used.</p>"},{"location":"troubleshooting/#access-to-proxy-server","title":"Access to proxy server","text":"<p>If a proxy server is used, you should check whether it can be accessed at port <code>3128</code> (default Squid port).</p> <p>For this, you can use standard networking tools (if available):</p> <ul> <li><code>nc</code>, ncat, a reimplementation of netcat:   <pre><code>nc -vz PROXY_IP 3128\n</code></pre></li> <li><code>telnet</code>:   <pre><code>telnet PROXY_IP 3128\n</code></pre></li> <li><code>tcptraceroute</code>:   <pre><code>sudo tcptraceroute PROXY_IP 3128\n</code></pre></li> </ul> <p>You will need to replace \"<code>PROXY_IP</code>\" in the commands above with the actual IP (or hostname) of the proxy server being used.</p>"},{"location":"troubleshooting/#determine-stratum-1","title":"Determine Stratum 1","text":"<p>Check which Stratum 1 servers are currently configured:</p> <pre><code>cvmfs_config showconfig software.eessi.io | grep CVMFS_SERVER_URL\n</code></pre> <p>Determine which Stratum 1 is currently being used by CernVM-FS:</p> <pre><code>$ sudo cvmfs_talk -i software.eessi.io host info\n  [0] http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io (unprobed)\n  [1] http://azure-us-east-s1.eessi.science/cvmfs/software.eessi.io (unprobed)\nActive host 0: http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io\n</code></pre> <p>In this case, the public Stratum 1 for EESSI in AWS <code>eu-central</code> is being used: <code>aws-eu-central-s1.eessi.science</code>.</p>"},{"location":"troubleshooting/#access-to-stratum-1","title":"Access to Stratum 1","text":"<p>If no proxy is being used (<code>CVMFS_HTTP_PROXY</code> is set to <code>DIRECT</code>, see also above), you should check whether the active Stratum 1 is directly accessible at port <code>80</code>.</p> <p>Again, you can use standard networking tools for this:</p> <p><pre><code>nc -vz aws-eu-central-s1.eessi.science 80\n</code></pre> <pre><code>telnet aws-eu-central-s1.eessi.science 80\n</code></pre> <pre><code>sudo tcptraceroute aws-eu-central-s1.eessi.science 80\n</code></pre></p>"},{"location":"troubleshooting/#download-from-stratum-1","title":"Download from Stratum 1","text":"<p>To see whether a Stratum 1 replica server can be used to download repository contents from, you can use <code>curl</code> to check whether the <code>.cvmfspublished</code> file is accessible ( this file must exist in every repository ):</p> <pre><code>S1_URL=\"http://aws-eu-central-s1.eessi.science\"\ncurl --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>If CernVM-FS is configured to use a proxy server, you should let <code>curl</code> use it too: <pre><code>P_URL=\"http://PROXY_IP:3128\"\nS1_URL=\"http://aws-eu-central-s1.eessi.science\"\ncurl --proxy ${P_URL} --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> or equivalently via the standard <code>http_proxy</code> environment variable that <code>curl</code> picks up on: <pre><code>S1_URL=\"http://aws-eu-central-s1.eessi.science\"\nhttp_proxy=\"PROXY_IP:3128\" curl --head ${S1_URL}/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre></p> <p>Make sure you replace \"<code>PROXY_IP</code>\" in the commands above with the actual IP (or hostname) of the proxy server.</p> <p>If you see a <code>200</code> HTTP return code in the first line of output produced by <code>curl</code>, access is working as it should:</p> <pre><code>HTTP/1.1 200 OK\n</code></pre> <p>If you see <code>403</code> as return code, then something is blocking the connection:</p> <pre><code>HTTP/1.1 403 Forbidden\n</code></pre> <p>In this case, you should check whether a firewall is being used, or whether an ACL in the Squid proxy configuration is the culprit.</p> <p>If you see <code>404</code> as return code, you made a typo in the <code>curl</code> command : <pre><code>HTTP/1.1 404 Not Found\n</code></pre> Maybe you forgot the '<code>.</code>' in <code>.cvmfspublished</code>?</p>"},{"location":"troubleshooting/#mounting","title":"Mounting problems","text":""},{"location":"troubleshooting/#autofs","title":"<code>autofs</code>","text":"<p>Keep in mind that (by default) CernVM-FS repositories are mounted via <code>autofs</code>.</p> <p>Hence, you should not rely on the output of <code>ls /cvmfs</code> to determine which repositories can be accessed with your current configuration, since they may not be mounted currently.</p> <p>You can check whether a specific repository is available by trying to access it directly:</p> <pre><code>ls /cvmfs/software.eessi.io\n</code></pre>"},{"location":"troubleshooting/#currently-mounted-repositories","title":"Currently mounted repositories","text":"<p>To check which CernVM-FS repositories are currently mounted, run: <pre><code>cvmfs_config stat\n</code></pre></p>"},{"location":"troubleshooting/#probing","title":"Probing","text":"<p>To check whether a repository can be mounted, you can try to probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre>"},{"location":"troubleshooting/#manual-mounting","title":"Manual mounting","text":"<p>If you can not get access to a repository via auto-mounting by <code>autofs</code>, you can try to manually mount it, since that may reveal specific error messages:</p> <pre><code>mkdir -p /tmp/cvmfs/eessi\nsudo mount -t cvmfs software.eessi.io /tmp/cvmfs/eessi\n</code></pre> <p>You can even try using the <code>cvmfs2</code> command directly to mount a repository: <pre><code>mkdir -p /tmp/cvmfs/eessi\nsudo /usr/bin/cvmfs2 -d -f \\\n    -o rw,system_mount,fsname=cvmfs2,allow_other,grab_mountpoint,uid=$(id -u cvmfs),gid=$(id -g cvmfs),libfuse=3 \\\n    software.eessi.io /tmp/cvmfs/eessi\n</code></pre> which prints lots of information for debugging (option <code>-d</code>).</p>"},{"location":"troubleshooting/#resources","title":"Insufficient resources","text":"<p>Keep in mind that the problems you observe may be the result of a shortage in resources, for example:</p> <ul> <li>Lack of sufficient memory, for example for the kernel file system cache, which will typically   lead to degrated (start-up) performance;</li> <li>Lack of sufficient disk space, for the CernVM-FS client cache, for the proxy server,   or for the private Stratum 1 replica server;</li> <li>Network latency issues, either within the local network (to the proxy server or Stratum 1 replica server),   or to the outside world (public Stratum 1 replica servers) &amp;endash; see also the Connectivity   section;</li> </ul>"},{"location":"troubleshooting/#caching","title":"Caching woes","text":"<p>Clean the cache to exclude any cache corruption as root cause of issues <pre><code>sudo cvmfs_config wipecache\n</code></pre></p> <p>Cache usage is included in the output of <pre><code>cvmfs_config stat -v software.eessi.io\n</code></pre></p> <p>Check consistency of the CernVM-FS cache directory <pre><code>sudo time cvmfs_fsck -j 8 /var/lib/cvmfs/shared\n</code></pre></p>"},{"location":"troubleshooting/#logs","title":"Logs","text":"<p>By default CernVM-FS logs to syslog, for example, <code>/var/log/messages</code> or <code>/var/log/syslog</code>. Scanning these logs for <code>cvmfs</code> may help to determine the root cause of an issue.</p> <p>For obtaining more detailed information, CernVM-FS provides the setting <code>CVMFS_DEBUGLOG</code>. If set as follows <pre><code>CVMFS_DEBUGLOG=/tmp/cvmfs_debug.log\n</code></pre> CernVM-FS logs more information to <code>/tmp/cvmfs_debug.log</code> after the command <pre><code>sudo cvmfs_config reload\n</code></pre> has been run. See CernVM-FS documentation / debug-logs for more information. Note that the debug log will log every operation in CVMFS and generates large files - it should be turned back off after capturing an issue.</p> <p>An interesting command for mounted repositories is <pre><code>attr -g logbuffer /cvmfs/software.eessi.io\n</code></pre> This will print the last syslog messages from this repositories without needing to access /var/log.</p>"},{"location":"troubleshooting/#general-tools","title":"General tools","text":"<p>If the repository <code>software.eessi.io</code> is mounted, the following command provides useful information and statistics <pre><code>cvmfs_config stat -v software.eessi.io\n</code></pre></p> <p>To verify whether the basic setup is sound, run <pre><code>sudo cvmfs_config chksetup\n</code></pre> which should print something like <pre><code>OK\n</code></pre> or a message indicating a problem such as <pre><code>Warning: autofs service is not running\n</code></pre></p> <p>Listing mounted repositories with <pre><code>cvmfs_config status\n</code></pre></p> <p>Printing non-empty configuration settings for a repository <pre><code>cvmfs_config showconfig -s software.eessi.io\n</code></pre></p> <p>Check if a CernVM-FS repository can be mounted <pre><code>cvmfs_config probe software.eessi.io\n</code></pre></p>"},{"location":"troubleshooting/#bandwidth","title":"Bandwidth","text":"<p>(maybe skip? or something for performance section?)</p> <ul> <li><code>iperf</code></li> </ul>"},{"location":"troubleshooting/#proxy","title":"Proxy","text":"<p><code>CVMFS_HTTP_PROXY</code></p> <p>https://cvmfs.readthedocs.io/en/stable/cpt-squid.html</p> <p>(the examples below didn't work ... squid.vega.pri seems not a known DNS name)</p> <p><code>http_proxy=http://squid.vega.pri:3128 curl -vs http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io/.cvmfspublished | cat -v</code></p> <p><pre><code>http_proxy=http://squid.vega.pri:3128 curl --head http://aws-eu-west1.stratum1.cvmfs.eessi-infra.org/cvmfs/pilot.eessi-hpc.org/.cvmfspublished\nHTTP/1.1 200 OK\n</code></pre> <pre><code>$ http_proxy=http://squid.vega.pri:3128 curl --head http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io/.cvmfspublished\nHTTP/1.1 403 Forbidden\n</code></pre></p>"},{"location":"troubleshooting/#incorrect-repository-configuration","title":"Incorrect repository configuration","text":"<p>(maybe skip?)</p> <p><code>/etc/cvmfs/keys</code></p> <p><code>/etc/cvmfs/default.local</code></p> <p><code>/etc/cvmfs/domain.d</code></p> <p>(next: Performance aspects of CernVM-FS)</p>"},{"location":"access/","title":"Accessing CernVM-FS repositories","text":"<ul> <li>Setting up a CernVM-FS client system</li> <li>Setting up a proxy server</li> <li>Setting up a Stratum 1 replica server</li> <li>Alternative ways to access CernVM-FS repositories</li> </ul>"},{"location":"access/alternatives/","title":"Alternative ways to access CernVM-FS repositories","text":"<p>While a native installation of CernVM-FS on the client system, along with a proxy server and/or Stratum 1 replica server for large-scale production setups, is recommended, there are other alternatives available for getting access to CernVM-FS repositories.</p> <p>We briefly cover some of these here, mostly to clarify that there are alternatives available, including some that do not require system administrator permissions.</p>"},{"location":"access/alternatives/#cvmfsexec","title":"<code>cvmfsexec</code>","text":"<p>Using <code>cvmfsexec</code>, mounting of CernVM-FS repositories as an unprivileged user is possible, without having CernVM-FS installed system-wide.</p> <p><code>cvmfsexec</code> supports multiple ways of doing this depending on the OS version and system configuration, more specifically whether or not particular features are enabled, like:</p> <ul> <li>FUSE mounting with <code>fusermount</code>;</li> <li>unprivileged user namespaces;</li> <li>unprivileged namespace fuse mounts;</li> <li>a <code>setuid</code> installation of Singularity 3.4+ (via <code>singcvmfs</code> which uses the <code>--fusemount</code> feature),   or an unprivileged installation of Singularity 3.6+;</li> </ul> <p>Start by cloning the <code>cvmfsexec</code> repository from GitHub, and change to the <code>cvmfsexec</code> directory:</p> <pre><code>git clone https://github.com/cvmfs/cvmfsexec.git\ncd cvmfsexec\n</code></pre> <p>Before using <code>cvmfsexec</code>, you first need to make a <code>dist</code> directory that includes CernVM-FS, configuration files, and scripts. For this, you can run the <code>makedist</code> script that comes with <code>cvmfsexec</code>:</p> <pre><code>./makedist default\n</code></pre> <p>With the <code>dist</code> directory in place, you can use <code>cvmfsexec</code> to run commands in an environment where a CernVM-FS repository is mounted.</p> <p>For example, we can run a script named <code>test_eessi.sh</code> that contains:</p> <pre><code>#!/bin/bash\n\nsource /cvmfs/software.eessi.io/versions/2023.06/init/bash\n\nmodule load TensorFlow/2.13.0-foss-2023a\n\npython -V\npython3 -c 'import tensorflow as tf; print(tf.__version__)'\n</code></pre> <p>which gives: <pre><code>$ ./cvmfsexec software.eessi.io -- ./test_eessi.sh\n\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/cvmfs-config.cern.ch\nCernVM-FS: loading Fuse module... done\nCernVM-FS: mounted cvmfs on /home/rocky/cvmfsexec/dist/cvmfs/software.eessi.io\n\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n\nPython 3.11.3\n2.13.0\n</code></pre></p> <p>By default, the CernVM-FS client cache directory will be located in <code>dist/var/lib/cvmfs</code>.</p> <p>For more information on <code>cvmfsexec</code>, see https://github.com/cvmfs/cvmfsexec.</p>"},{"location":"access/alternatives/#apptainer-with-fusemount","title":"Apptainer with <code>--fusemount</code>","text":"<p>If Apptainer is available, you can get access to a CernVM-FS repository by using a container image that includes the CernVM-FS client component (see for example the Docker recipe for the client container used in EESSI, which is available here).</p> <p>Using the <code>--fusemount</code> option you can specify that a CernVM-FS repository should be mounted when starting the container. For example for EESSI, you should use:</p> <pre><code>apptainer ... --fusemount \"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\" ...\n</code></pre> <p>There are a couple of caveats here:</p> <ul> <li> <p>If the configuration for the CernVM-FS repository is provided via the <code>cvmfs-config</code> repository,   you need to instruct Apptainer to also mount that, by using the <code>--fusemount</code> option twice: once for   the <code>cvmfs-config</code> repository, and once for the target repository itself:   <pre><code>FUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer ... --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" ...\n</code></pre></p> </li> <li> <p>Next to mounting CernVM-FS repositories, you also need to bind mount local writable directories   to <code>/var/run/cvmfs</code>, since CernVM-FS needs write access in those locations (for the CernVM-FS client cache):   <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\napptainer ... --fusemount ...\n</code></pre></p> </li> </ul> <p>To try this, you can use the EESSI client container that is available in Docker Hub, to start an interactive shell in which EESSI is available, as follows:</p> <pre><code>mkdir -p /tmp/$USER/{var-lib-cvmfs,var-run-cvmfs}\nexport APPTAINER_BIND=\"/tmp/$USER/var-run-cvmfs:/var/run/cvmfs,/tmp/$USER/var-lib-cvmfs:/var/lib/cvmfs\"\nFUSEMOUNT_CVMFS_CONFIG=\"container:cvmfs2 cvmfs-config.cern.ch /cvmfs/cvmfs-config.cern.ch\"\nFUSEMOUNT_EESSI=\"container:cvmfs2 software.eessi.io /cvmfs/software.eessi.io\"\napptainer shell --fusemount \"${FUSEMOUNT_CVMFS_CONFIG}\" --fusemount \"${FUSEMOUNT_EESSI}\" docker://ghcr.io/eessi/client-pilot:centos7\n</code></pre>"},{"location":"access/alternatives/#alien-cache","title":"Alien cache","text":"<p>An alien cache can be used, optionally in combination with preloading, as another alternative, typically in combination with using a container image or unprivileged user namespaces.</p> <p>For more information, see the Alien cache subsection in the next part of the tutorial.</p> <p>(next: Configuration on HPC systems)</p>"},{"location":"access/client/","title":"CernVM-FS client system","text":"<p>The recommended way to gain access to CernVM-FS repositories is to set up a system-wide native installation of CernVM-FS on the client system(s), which comes down to:</p> <ul> <li>Installing the client component of CernVM-FS;</li> <li>Creating a minimal client configuration file (<code>/etc/cvmfs/default.local</code>);</li> <li>Completing the client setup by:<ul> <li>Creating a<code>cvmfs</code> user account and group;</li> <li>Creating the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories;</li> <li>Configuring <code>autofs</code> to enable auto-mounting of repositories (recommended).</li> </ul> </li> </ul> <p>For repositories that are not included in the default CernVM-FS configuration you also need to provide some additional information specific to those repositories in order to access them.</p> <p>This is not a production-ready setup (yet)!</p> <p>While these basic steps are enough to gain access to CernVM-FS repositories, this is not sufficient to obtain a production-ready setup.</p> <p>This is especially true on HPC infrastructure that typically consists of a large number of worker nodes on which software provided by one or more CernVM-FS repositories will be used.</p> <p>After covering the basic client setup in this section, we will outline how to make accessing of CernVM-FS repositories more reliable and performant, by also setting up a proxy server and CernVM-FS Stratum 1 replica server.</p>"},{"location":"access/client/#installing-cernvm-fs-client","title":"Installing CernVM-FS client","text":"<p>Start with installing the <code>cvmfs</code> package which provides the CernVM-FS client component:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS client package\nsudo yum install -y cvmfs\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS client package\nsudo apt install -y cvmfs\n</code></pre> <p>If none of the available <code>cvmfs</code> packages are compatible with your system, you can also build CernVM-FS from source.</p>"},{"location":"access/client/#minimal_configuration","title":"Minimal client configuration","text":"<p>Next to installing the CernVM-FS client, you should also create a minimal configuration file for it.</p> <p>This is typically done in <code>/etc/cvmfs/default.local</code>, which should contain something like:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\" # a single node setup, not a cluster\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>More information on the structure of <code>/etc/cvmfs</code> and supported configuration settings is available in the CernVM-FS documentation.</p> Client profile setting (click to expand) <p>With <code>CVMFS_CLIENT_PROFILE=\"single\"</code> we specify that this CernVM-FS client should:</p> <ul> <li>Use the proxy server specified via <code>CVMFS_HTTP_PROXY</code>, if that configuration setting is defined;</li> <li>Directly connect to a Stratum-1 replica server that provides the repository being used if no proxy server   is specified via <code>CVMFS_HTTP_PROXY</code>.</li> </ul> <p>As an alternative to defining <code>CVMFS_CLIENT_PROFILE</code>, you can also set <code>CVMFS_HTTP_PROXY</code> to <code>DIRECT</code> to specify that no proxy server should be used by CernVM-FS:</p> <pre><code>CVMFS_HTTP_PROXY=\"DIRECT\"\n</code></pre> <p>We will get back to <code>CVMFS_HTTP_PROXY</code> later when setting up a proxy server.</p> Maximum size of client cache (click to expand) <p>The <code>CVMFS_QUOTA_LIMIT</code> configuration setting specifies the maximum size of the CernVM-FS client cache (in MBs).</p> <p>In the example above, we specify that no more than ~10GB should be used for the client cache.</p> <p>When the specified quota limit is reached, CernVM-FS will automatically remove files from the cache according to the Least Recently Used (LRU) policy, until half of the maximum cache size has been freed.</p> <p>The location of the cache directory can be controlled by <code>CVMFS_CACHE_BASE</code> if needed (default: <code>/var/lib/cvmfs</code>), but must be a on a local file system of the client, not a network file system that can be modified by multiple hosts.</p> <p>Using a directory in a RAM disk (like <code>/dev/shm</code>) for the CernVM-FS client cache can be considered if enough memory is available in the client system, which would help reduce latency and start-up performance of software.</p> <p>For more information on cache-related configuration settings, see the CernVM-FS documentation.</p>"},{"location":"access/client/#configuration_hierarchy","title":"Hierarchy of configuration files","text":"<p>CernVM-FS can be configured through a hierarchy of configuration files (sometimes also referred to as parameter files), which can be located under either <code>/etc/cvmfs</code>, or the CernVM-FS configuration repository that is being used (there can be only one), which lives under <code>/cvmfs</code> and is determined by the <code>CVMFS_CONFIG_REPOSITORY</code> configuration setting.</p> <p>There are 3 levels of configuration files: general, domain-specific, and repository-specific.</p> <p>Each CernVM-FS configuration file has either <code>.conf</code> or <code>.local</code> as file extension.</p>"},{"location":"access/client/#order-of-configuration-files","title":"Order of configuration files","text":"<p>A CernVM-FS configuration file is sourced to set the configuration settings (a.k.a. parameters) it specifies, in the order outlined below:</p> <ul> <li>By level: first general, then domain-specific, finally repository-specific;</li> <li>Within each level:<ul> <li><code>.conf</code> before <code>.local</code>;</li> <li>CernVM-FS configuration repository before <code>/etc/cvmfs</code> (except for general level);</li> </ul> </li> </ul> <p>As a result, a configuration file that is picked up later can override configuration settings that were specified in an earlier consider configuration file.</p> <p>Concrete example: the settings in the general <code>/etc/cvmfs/default.local</code> configuration file are overridden by those specified in the domain-specific configuration file <code>/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf</code> (which is located in the default CernVM-FS configuration repository <code>cvmfs-config.cern.ch</code>).</p>"},{"location":"access/client/#cfg_general","title":"General level","text":"<p>At the general level, the following configuration files are considered (in order):</p> <ul> <li><code>/etc/cvmfs/default.conf</code></li> <li><code>/etc/cvmfs/default.d/*.conf</code> (in alphabetical order)</li> <li><code>$CVMFS_CONFIG_REPOSITORY/etc/cvmfs/default.conf</code></li> <li><code>/etc/cvmfs/default.local</code></li> </ul>"},{"location":"access/client/#cfg_domain","title":"Domain-specific level","text":"<p>At the domain-specific level, the following configuration files are considered (in order):</p> <ul> <li><code>$CVMFS_CONFIG_REPOSITORY/etc/cvmfs/domain.d/DOMAIN.conf</code></li> <li><code>/etc/cvmfs/domain.d/DOMAIN.conf</code></li> <li><code>/etc/cvmfs/domain.d/DOMAIN.local</code></li> </ul> <p>where \"<code>DOMAIN</code>\" is replaced by the domain of the CernVM-FS repository being considered, like <code>eessi.io</code> for <code>software.eessi.io</code>.</p>"},{"location":"access/client/#cfg_repository","title":"Repository-specific level","text":"<p>At the repository-specific level, the following configuration files are considered (in order):</p> <ul> <li><code>$CVMFS_CONFIG_REPOSITORY/etc/cvmfs/config.d/&lt;your_repository&gt;.conf</code></li> <li><code>/etc/cvmfs/config.d/&lt;your_repository&gt;.conf</code></li> <li><code>/etc/cvmfs/config.d/&lt;your_repository&gt;.local</code></li> </ul>"},{"location":"access/client/#show-configuration","title":"Show configuration","text":"<p>To show all configuration settings in alphabetical order, including by which configuration file it got set, use <code>cvmfs_config showconfig</code>, for example:</p> <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre> <p>For <code>CVMFS_QUOTA_LIMIT</code>, you should see this in the output:</p> <pre><code>CVMFS_QUOTA_LIMIT=10000    # from /etc/cvmfs/default.local\n</code></pre> <p>See also Inspecting repository configuration.</p>"},{"location":"access/client/#completing-the-client-setup","title":"Completing the client setup","text":"<p>To complete the setup of the CernVM-FS client component, we need to make sure that a <code>cvmfs</code> service account and group are present on the system, and the <code>/cvmfs</code> and <code>/var/lib/cvmfs</code> directories exist with the correct ownership and permissions.</p> <p>This should be taken care of by the post-install script that is run when installing the <code>cvmfs</code> package, so you will only need to take action on these aspects if you were installing the CernVM-FS client from source.</p> <p>In addition, it is recommended to update the <code>autofs</code> configuration to enable auto-mounting of CernVM-FS repositories, and to make sure the <code>autofs</code> service is running.</p> <p>All these actions can be performed in one go by running the following command:</p> <pre><code>sudo cvmfs_config setup\n</code></pre> <p>Additional options can be passed to the <code>cvmfs_config setup</code> command to disable some of the actions, like <code>nouser</code> to not create the <code>cvmfs</code> user and group, or <code>noautofs</code> to not update the <code>autofs</code> configuration.</p>"},{"location":"access/client/#autofs","title":"Recommendations for <code>autofs</code>","text":"<p>It is recommended to configure <code>autofs</code> to never unmount repositories due to inactivity, since that can cause problems in specific situations.</p> <p>This can be done by setting additional options in <code>/etc/sysconfig/autofs</code> (on RHEL-based Linux distributions) or <code>/etc/default/autofs</code> (on Debian-based distributions):</p> <pre><code>OPTIONS=\"--timeout 0\"\n</code></pre> <p>The default <code>autofs</code> timeout is typically 5 minutes (300 seconds), which is usually specified in <code>/etc/autofs.conf</code>.</p> Warning when using Slurm's <code>job_container/tmpfs</code> plugin with <code>autofs</code> (click to expand) <p>Slurm versions up to 23.02 had issues when the <code>job_container/tmpfs</code> plugin was being used in combination with <code>autofs</code>. More information can be found at the Slurm bug tracker and the CernVM-FS forum.</p> <p>Slurm version 23.02 includes a fix by providing a <code>Shared</code> option for the <code>job_container/tmpfs</code> plugin, which allows it to work with <code>autofs</code>.</p>"},{"location":"access/client/#using-static-mounts","title":"Using static mounts","text":"<p>If you prefer not to use <code>autofs</code>, you will need to use static mounting, by either:</p> <ul> <li> <p>Manually mounting the CernVM-FS repositories you want to use, for example:   <pre><code>sudo mkdir -p /cvmfs/software.eessi.io\nsudo mount -t cvmfs software.eessi.io /cvmfs/software.eessi.io\n</code></pre></p> </li> <li> <p>Updating <code>/etc/fstab</code> to ensure that the CernVM-FS repositories are mounted at boot time.</p> </li> </ul> <p>Configuring <code>autofs</code> to never unmount due to inactivity is preferable to using static mounts, because the latter requires that every repository is mounted individually, even if is already known in your CernVM-FS configuration. When using <code>autofs</code> you can access all repositories that are known to CernVM-FS through its active configuration.</p> <p>For more information on mounting repositories, see the CernVM-FS documentation.</p>"},{"location":"access/client/#checking-client-setup","title":"Checking client setup","text":"<p>To ensure that the setup of the CernVM-FS client component is valid, you can run:</p> <pre><code>sudo cvmfs_config chksetup\n</code></pre> <p>You should see <code>OK</code> as output of this command.</p>"},{"location":"access/client/#default-repositories","title":"Default repositories","text":"<p>The default configuration of CernVM-FS, provided by the <code>cvmfs-config-default</code> package, provides the public keys and configuration for a number of commonly used CernVM-FS repositories.</p> <p>One particular repository included in the default CernVM-FS configuration is <code>cvmfs-config.cern.ch</code>, which is a CernVM-FS config repository that provides public keys and configuration for additional flagship CernVM-FS repositories, like <code>software.eessi.io</code>:</p> <pre><code>$ ls /cvmfs/cvmfs-config.cern.ch/etc/cvmfs\ncommon.conf  config.d  default.conf  domain.d  keys\n\n$ find /cvmfs/cvmfs-config.cern.ch/etc/cvmfs -type f -name '*eessi*'\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/domain.d/eessi.io.conf\n/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io/eessi.io.pub\n</code></pre> <p>That means we now already have access to the EESSI CernVM-FS repository:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nREADME.eessi  host_injections  versions\n</code></pre>"},{"location":"access/client/#inspecting_configuration","title":"Inspecting repository configuration","text":"<p>To check whether a specific CernVM-FS repository is accessible, we can probe it:</p> <pre><code>$ cvmfs_config probe software.eessi.io\nProbing /cvmfs/software.eessi.io... OK\n</code></pre> <p>To view the configuration for a specific repository, use <code>cvmfs_config showconfig</code>: <pre><code>cvmfs_config showconfig software.eessi.io\n</code></pre></p> <p>To check the active configuration for a specific repository used by the running CernVM-FS instance, use <code>cvmfs_talk -i &lt;repo&gt; parameters</code> (which requires admin privileges):</p> <pre><code>sudo cvmfs_talk -i software.eessi.io parameters\n</code></pre> <p><code>cvmfs_talk</code> requires that the repository is currently mounted. If not, you will see an error like this:</p> <pre><code>$ sudo cvmfs_talk -i software.eessi.io parameters\nSeems like CernVM-FS is not running in /var/lib/cvmfs/shared (not found: /var/lib/cvmfs/shared/cvmfs_io.software.eessi.io)\n</code></pre>"},{"location":"access/client/#accessing-a-repository","title":"Accessing a repository","text":"<p>To access the contents of the repository, just use the corresponding subdirectory as if it were a local filesystem.</p> <p>While the contents of the files you are accessing are not actually available on the client system the first time they are being accessed, CernVM-FS will automatically downloaded them in the background, providing the illusion that the whole repository is already there.</p> <p>We like to refer to this as \"streaming\" of software installations, much like streaming music or video services.</p> <p>To start using EESSI just source the initialisation script included in the repository:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>You may notice some \"lag\" when files are being accessed, or not, depending on the network latency.</p>"},{"location":"access/client/#additional-repositories","title":"Additional repositories","text":"<p>To access additional CernVM-FS repositories beyond those that are available by default, you will need to:</p> <ul> <li>Add the public keys for those repositories into a domain-specific subdirectory of <code>/etc/cvmfs/keys/</code>;</li> <li>Add the configuration for those repositories into <code>/etc/cvmfs/domain.d</code> (domain-specific) or <code>/etc/cvmfs/config.d</code> (repository-specific).</li> </ul> <p>Examples are available in the <code>etc/cvmfs</code> subdirectory of the config-repo GitHub repository.</p> <p>(next: Squid proxy server)</p>"},{"location":"access/proxy/","title":"Squid proxy server","text":"<p>As a first step towards a production-ready CernVM-FS setup we can install a Squid forward proxy server, which is strongly recommended in the context of HPC systems.</p> <p>The proxy server will (often dramatically) reduce the latency for client systems for accessing CernVM-FS repositories, and hence significantly improve start-up performance of software provided via CernVM-FS. In addition, it reduces the load on the Stratum 1 replica servers that support the repositories being used.</p> <p>This is particularly important when running large-scale MPI software, since the software binary and all the libraries it requires must be available on all worker nodes being employed before the software can start running.</p>"},{"location":"access/proxy/#general-recommendations","title":"General recommendations","text":"<p>The proxy server should have a 10Gbit link to the client systems, a sufficiently powerful CPU, a decent amount of memory for the kernel cache (tens of GBs), and fast local storage (SSD or NVMe).</p> <p>It is strongly recommended to have at least two proxy servers available, to have some redundancy available in case of unexpected problems, or during a maintenance window.</p> <p>As a rule of thumb, it is recommended to have (at least) one proxy server for every couple of hundred worker nodes (100-500).</p> <p>Note that the load on the proxy servers used by CernVM-FS is highly dependent on the workload mix on the client systems.</p>"},{"location":"access/proxy/#proxy-server-setup","title":"Proxy server setup","text":""},{"location":"access/proxy/#installation","title":"Installation","text":"<p>First, install the <code>squid</code> package using your OS package manager:</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code>sudo yum install -y squid\n</code></pre> <pre><code>sudo apt install -y squid\n</code></pre>"},{"location":"access/proxy/#configuration","title":"Configuration","text":"<p>Create a configuration file for the Squid proxy in <code>/etc/squid/squid.conf</code>.</p> <p>You can use the following template for this:</p> <pre><code># List of local IP addresses (separate IPs and/or CIDR notation) allowed to access your local proxy\nacl local_nodes src YOUR_CLIENT_IPS\n\n# Destination domains that are allowed\nacl stratum_ones dstdomain .YOURDOMAIN.ORG\n\n# Squid port\nhttp_port 3128\n\n# Deny access to anything which is not part of our stratum_ones ACL.\nhttp_access deny !stratum_ones\n\n# Only allow access from our local machines\nhttp_access allow local_nodes\nhttp_access allow localhost\n\n# Finally, deny all other access to this proxy\nhttp_access deny all\n\nminimum_expiry_time 0\nmaximum_object_size 1024 MB\n\n# proxy memory cache of 1GB\ncache_mem 1024 MB\nmaximum_object_size_in_memory 128 KB\n# 50 GB disk cache\ncache_dir ufs /var/spool/squid 50000 16 256\n</code></pre> <p>In this template, you must change two things in the Access Control List (ACL) settings:</p> <p>1) Specify which client systems can access your proxy by replacing \"<code>YOUR_CLIENT_IPS</code>\" with the corresponding IP range, using CIDR notation;</p> <p>2) Make sure that the proxy server allows access to the Stratum 1 replica servers that are relevant for the CernVM-FS repositories    you are using, by replacing \"<code>.YOURDOMAIN.ORG</code>\" with domain name for the Stratum 1 replica servers    (see also the Squid ACL documentation).</p> <p>For example, to allow connecting to the EESSI Stratum 1 replica servers, use:</p> <pre><code>acl stratum_ones dstdomain .eessi.science\n</code></pre> <p>Note that this configuration assumes that port 3128 is accessible on the proxy server.</p> <p>To check your Squid configuration, use:</p> <pre><code>sudo squid -k parse\n</code></pre> <p>If no warnings or errors are printed by this command, you should be all set (assuming that the ACLs are set correctly).</p> <p>For more information on configuring a Squid proxy, see the CernVM-FS documentation,</p>"},{"location":"access/proxy/#starting-the-service","title":"Starting the service","text":"<p>To start the Squid and enable it on booting the proxy server, run:</p> <pre><code>sudo systemctl start squid\nsudo systemctl enable squid\n</code></pre> <p>To check the status of the Squid, you can use:</p> <pre><code>sudo systemctl status squid\n</code></pre>"},{"location":"access/proxy/#client-system-configuration","title":"Client system configuration","text":"<p>To make a CernVM-FS client system use the proxy server, the <code>/etc/cvmfs/default.local</code> configuration file on the client system should be updated to include:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nCVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>To apply the change we need to reload the CernVM-FS configuration on the client system:</p> <pre><code>sudo cvmfs_config reload\n</code></pre> <p>You can test the new configuration and verify whether the proxy is indeed being used by the client system via <code>cvmfs_config stat</code>:</p> <pre><code>ls /cvmfs/software.eessi.io\ncvmfs_config stat -v software.eessi.io\n</code></pre> <p>We first inspect the contents of the repository using <code>ls</code> to make sure that the repository is mounted, which is assumed by <code>cvmfs_config stat</code>.</p> <p>The output of the <code>stat</code> command should look something like this:</p> <p>Note the <code>Connection</code> line, which clearly shows that the proxy server is used (and is working):</p> <pre><code>Connection: .../software.eessi.io through proxy http://PROXY_IP:3128 (online)\n</code></pre> <p>To check whether the proxy is working as intended you can use <code>curl</code> to try to access the <code>.cvmfspublished</code> file in the root of the repository on a Stratum 1, for example:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nhttp_proxy=http://PROXY_IP:3128 curl --head http://aws-eu-central-s1.eessi.science/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>The first output line of this command should be:</p> <pre><code>HTTP/1.1 200 OK\n</code></pre>"},{"location":"access/proxy/#cleanup_proxy","title":"Cleanup to prepare for Stratum 1","text":"<p>To prepare for the next tutorial section on setting up a private Stratum 1 replica server, comment out the <code>CVMFS_HTTP_PROXY</code> line in <code>/etc/cvmfs/default.local</code> by prefixing it with a hash (<code>#</code>):</p> <pre><code>#CVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>The full contents of <code>/etc/cvmfs/default.local</code> on the client system should again be as shown below, like it was initially created when configuring the client:</p> <pre><code>CVMFS_CLIENT_PROFILE=\"single\"\nCVMFS_QUOTA_LIMIT=10000\n</code></pre> <p>Do not forget to also reload the CernVM-FS client configuration: <pre><code>sudo cvmfs_config reload\n</code></pre></p> <p>We will later reconfigure the proxy server so it can be used together with our private Stratum 1.</p> <p>(next: Setting up a Stratum 1 replica server)</p>"},{"location":"access/stratum1/","title":"Private Stratum 1 replica server","text":"<p>In this section of the tutorial, we will set up a Stratum 1 replica server, which is the next step towards a production-ready CernVM-FS setup.</p>"},{"location":"access/stratum1/#cernvm-fs-as-cdn","title":"CernVM-FS as CDN","text":"<p>The content of CernVM-FS repositories is served by a set of Stratum 1 replica servers (sometimes also called mirror servers), which together with the central Stratum 0 server and the proxy servers can be seen as a content delivery network (CDN).</p> <p>A Stratum 1 replica server is a standard web server that uses the CernVM-FS server tools to provide a full mirror of one or more CernVM-FS repositories, which are served and managed through a central Stratum 0 server.</p> <p>The figure below shows the CernVM-FS network for repositories in the <code>cern.ch</code> domain, including the Stratum 1 replica servers which are spread all across the world, and a distributed hierarchy of proxy servers which fetch content from the closest Stratum 1.</p> <p> </p>"},{"location":"access/stratum1/#motivation","title":"Motivation","text":"<p>Next to the public Stratum 1 servers that are operated by the maintainers of a CernVM-FS repository, you can also set up your own \"private\" Stratum 1 replica server in your local network.</p> <p>In the context of using CernVM-FS on HPC infrastructure this brings the following benefits:</p> <ul> <li>To improve the overall reliability of the setup, for example in case of (temporary) loss of connectivity to the public Stratum 1 replica servers;</li> <li>To reduce the load on the public Stratum 1 servers;</li> <li>To mitigate the impact of poor network bandwidth to the closest public Stratum 1 server;</li> <li>To improve the latency and hence start-up time of software in situations where the cache of the proxy servers has insufficient capacity;</li> </ul>"},{"location":"access/stratum1/#recommendations","title":"Recommendations","text":"<p>When setting up a Stratum 1 replica server, you should take the following recommendations into account:</p> <ul> <li>A RAID-protected low latency storage setup (like SSD   or NVMe) should be used,   because the CernVM-FS server component will run lots of <code>stat</code> system calls against it.</li> <li>An <code>ext3</code> or <code>ext4</code> file system   is preferred (rather than XFS).</li> <li>A standard Apache web server should be installed, which should be close to the   low latency storage. Directory listing is not required.</li> <li>HTTP connections to port <code>80</code> must be possible.</li> </ul> Recommendations on monitoring (click to expand) <p>It is strongly recommended to actively monitor a Stratum 1 replica server, in particular:</p> <ul> <li>CPU usage;</li> <li>disk usage;</li> <li>I/O load;</li> <li>network bandwidth and latency;</li> <li>log messages produced in syslog;</li> </ul> <p>The <code>cvmfs-servermon</code> package can be used to watch for problems in every repository\u2019s <code>.cvmfs_status.json</code> status file.</p> <p>See also the CernVM-FS documentation.</p> Recommendations for a high-availability setup (click to expand) <p>To create a high-availability setup, it is not recommended to use two or more separate Stratum 1 replica servers in a single round-robin service.</p> <p>Since they will be updated at different rates, that would cause errors when a client sees an updated catalog from one Stratum 1, but then tries to read corresponding data files from another that does not yet have the files.</p> <p>Instead, different Stratum 1 replica servers should either be separately configured on the clients, or a pair can be configured  as a high availability active/standby pair using the <code>cvmfs-hastratum1</code> package. </p> <p>An active/standby pair can also be managed by switching a DNS name between two different servers.</p> Recommendations for a public Stratum 1 replica server (click to expand) <p>For a public Stratum 1 replica server, it is recommended to install a Squid frontend in front of the Stratum 1, which should be configured as a reverse proxy, and installed on the same system as the web server, to reduce the number of points of failure. The optimized <code>frontier-squid</code> distribution is recommended. For more information, see the CernVM-FS configuration.</p> <p>Alternatively, separate Squid proxy server machines can be configured in a round-robin DNS configuration and each forward to the Apache server. Note however that if any of them are down the entire service will be considered down by CernVM-FS clients. The impact of this can be mitigated through front end hardware load balancer that quickly takes a system that is down out of service.</p> Recommendations on garbage collection (click to expand) <p>If any CernVM-FS repositories being replicated have garbage collection enabled, the Stratum 1 also needs to run garbage collection in order to prevent the disk space usage from growing rapidly.</p> <p>See the CernVM-FS documentation for more details.</p> Using S3-compatible storage (Amazon S3, Azure Blob, Ceph) <p>CernVM-FS can store data directly to S3-compatible storage systems, such as Amazon S3, Azure Blob, or Ceph.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"access/stratum1/#setup-procedure","title":"Setup procedure","text":"<p>To set up a Stratum 1 replica server and configure it to replicate a particular CernVM-FS repository, you should:</p> <ul> <li>Install the <code>cvmfs-server</code> package;</li> <li>Add the public key of the CernVM-FS repository you want to replicate to <code>/etc/cvmfs/keys/</code>;</li> <li>Create the repository replica;</li> <li>Run the initial synchronisation;</li> <li>Configure <code>cron</code> to perform periodic synchronisation;</li> </ul> <p>In the sections below, we will set up a Stratum 1 replica server for the EESSI CernVM-FS repository <code>software.eessi.io</code>.</p>"},{"location":"access/stratum1/#installing-cernvm-fs-server","title":"Installing CernVM-FS server","text":"<p>Start with installing the <code>cvmfs-server</code> package which provides the CernVM-FS server tools.</p> <p>Although we won't actually use the functionality that requires it, we also need to install a package that provides the <code>mod_wsgi</code> Apache adapter module.</p> For RHEL-based Linux distros (incl. CentOS, Rocky, Fedora, ...)For Debian-based Linux distros (incl. Ubuntu) <pre><code># install cvmfs-release package to add yum repository\nsudo yum install -y https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest.noarch.rpm\n\n# install CernVM-FS server package\nsudo yum install -y cvmfs-server\n\n# install mod_wsgi Apache adapter module\n# (on versions older than equivalent to RHEL8, install mod_wsgi instead)\nsudo yum install -y python3-mod_wsgi\n</code></pre> <pre><code># install cvmfs-release package to add apt repository\nsudo apt install lsb-release\ncurl -OL https://ecsft.cern.ch/dist/cvmfs/cvmfs-release/cvmfs-release-latest_all.deb\nsudo dpkg -i cvmfs-release-latest_all.deb\nsudo apt update\n\n# install CernVM-FS server package + the required mod-wsgi\nsudo apt install -y cvmfs-server\n\n# install mod_wsgi Apache adapter module\n# (on Ubuntu versions older than 22.04, install libapache2-mod-wsgi instead)\nsudo apt install -y libapache2-mod-wsgi-py3\n</code></pre>"},{"location":"access/stratum1/#adding-repository-public-key","title":"Adding repository public key","text":"<p>Add the public key for the repositories in the <code>eessi.io</code> domain to <code>/etc/cvmfs/keys</code>:</p> <pre><code>sudo mkdir -p /etc/cvmfs/keys/eessi.io/\nsudo cp eessi.io.pub /etc/cvmfs/keys/eessi.io/\n</code></pre> <p>You can get the contents for <code>eessi.io.pub</code> from the default CernVM-FS config repository on a CernVM-FS client system at:</p> <pre><code>/cvmfs/cvmfs-config.cern.ch/etc/cvmfs/keys/eessi.io/eessi.io.pub\n</code></pre>"},{"location":"access/stratum1/#creating-repository-replica","title":"Creating repository replica","text":"<p>To create the repository replica, we need to use run the <code>cvmfs_server add-replica</code> command.</p>"},{"location":"access/stratum1/#synchronisation-server","title":"Synchronisation server","text":"<p>We will need to specify the server that should be used for synchronising the repository contents. This can either be the Stratum 0 server, or a public Stratum 1 replica server that was set up to be used for repository synchronisation (by having a <code>.cvmfs_master_replica</code> file in the HTTP root directory).</p> <p>For EESSI we should use <code>aws-eu-west-s1-sync.eessi.science</code> as synchronisation server.</p>"},{"location":"access/stratum1/#disabling-the-geoapi","title":"Disabling the GeoAPI","text":"<p>Before creating the replica, we first need to disable the Geo API service in the CernVM-FS server configuration, to avoid getting this error when creating the replica:</p> <pre><code>Installing GeoIP Database... CVMFS_GEO_LICENSE_KEY not set\nfail\n</code></pre> <p>The Geo API service enables client systems to automatically sort Stratum 1 replica servers geographically, so the CernVM-FS client component can prioritize connecting to the closest one.</p> <p>This is really only relevant for public Stratum 1 replica servers, not a private Stratum 1 replica server that is only accessible from within the local network, like the one we are setting up here.</p> <p>To disable the Geo API service, set <code>CVMFS_GEO_DB_FILE</code> to <code>NONE</code> in <code>/etc/cvmfs/server.local</code>:</p> <pre><code>echo 'CVMFS_GEO_DB_FILE=NONE' | sudo tee -a /etc/cvmfs/server.local\n</code></pre>"},{"location":"access/stratum1/#creating-replica","title":"Creating replica","text":"<p>To actually create the replica, run the <code>cvmfs_server add-replica</code> command as follows, specifying that the current user account should be the repository owner via <code>-o $USER</code>:</p> <pre><code>sync_server='aws-eu-west-s1-sync.eessi.science'\nrepo='software.eessi.io'\nkey_dir='/etc/cvmfs/keys/eessi.io'\nsudo cvmfs_server add-replica -o $USER http://${sync_server}/cvmfs/${repo} ${key_dir}\n</code></pre> Starting Apache (click to expand) <p>If creating the replica fails with:</p> <pre><code>Apache must be installed and running\n</code></pre> <p>try starting the <code>httpd</code> service first:</p> <pre><code>sudo systemctl start httpd.service\nsudo systemctl enable httpd.service\n</code></pre>"},{"location":"access/stratum1/#initial-synchronisation","title":"Initial synchronisation","text":"<p>After creating the replica, we should trigger the initial synchronisation of the repository replica, using the <code>cvmfs_server snapshot</code> command:</p> <pre><code>cvmfs_server snapshot software.eessi.io\n</code></pre> <p>Time for a coffee...</p> <p>Since this will download the full repository contents from the synchronisation server that was specified when creating the repository replica, the initial synchronisation may take a while.</p> <p>The time required for the initial synchronisation is heavily dependent on the size of the repository, and the available network latency to the synchronisation server.</p>"},{"location":"access/stratum1/#periodic-synchronisation","title":"Periodic synchronisation","text":"<p>To ensure that updates to the contents of the CernVM-FS repository are synchronised automatically to the Stratum-1 replica server, we should set up a cron job to do periodic synchronisation by running <code>cvmfs_server snapshot -a</code>.</p>"},{"location":"access/stratum1/#log-rotation","title":"Log rotation","text":"<p>Before setting up a cron job, we first need to configure log rotation, or running <code>snapshot -a</code> will fail with: <pre><code>/etc/logrotate.d/cvmfs does not exist!\n</code></pre></p> <p>Create <code>/etc/logrotate.d/cvmfs</code> with the following contents:</p> <pre><code>/var/log/cvmfs/*.log {\n    weekly\n    missingok\n    notifempty\n}\n</code></pre>"},{"location":"access/stratum1/#cron-job","title":"Cron job","text":"<p>To synchronize all active replica repositories every 5 minutes, we can create a cron job <code>/etc/cron.d/cvmfs_stratum1_snapshot</code> that runs <code>cvmfs_server snapshot -a -i</code>:</p> <pre><code>*/5 * * * * OWNER output=$(/usr/bin/cvmfs_server snapshot -a -i 2&gt;&amp;1) || echo \"$output\"\n</code></pre> <p>In here, you must replace \"<code>OWNER</code>\" with the account name of the repository owner (cfr. the <code>-o $USER</code> option used in the <code>add-replica</code> command above).</p> <p>The <code>-a</code> option enables synchronisation of all active replica repositories, while <code>-i</code> indicates that that repositories for which an initial snapshot has not been run should be skipped.</p> <p>To verify that periodic synchronisation is working correctly, check the contents of the log file:</p> <pre><code>/var/log/cvmfs/snapshots.log\n</code></pre>"},{"location":"access/stratum1/#more-information","title":"More information","text":"<p>For more information on the setup and configuration of a Stratum 1 replica server, see the CernVM-FS documentation, in particular the following sections:</p> <ul> <li>Notable CernVM-FS Server Locations and Files</li> <li>CernVM-FS Server Infrastructure</li> </ul>"},{"location":"access/stratum1/#using-the-private-stratum-1","title":"Using the private Stratum 1","text":"<p>To actually use the \"private\" Stratum 1 replica server that has been set up we need to change the configuration on each CernVM-FS client system.</p> <p>Initially, we will use only the private Stratum 1 replica server, without a proxy server.</p> <p>Remove <code>CVMFS_HTTP_PROXY</code> from client configuration</p> <p>Do make sure that the <code>CVMFS_HTTP_PROXY</code> line is removed from the CernVM-FS configuration file <code>/etc/cvmfs/default.local</code> on the client system, and that the CernVM-FS configuration was reloaded (with <code>sudo cvmfs_config reload</code>), as was instructed here.</p> <p>After we have verified that the Stratum 1 is used by the client system, we will bring the proxy server back in the game, and demonstrate how to use both the proxy server and the Stratum 1 replica server.</p>"},{"location":"access/stratum1/#only-private-stratum-1","title":"Only private Stratum 1","text":""},{"location":"access/stratum1/#client-configuration","title":"Client configuration","text":"<p>The <code>CVMFS_SERVER_URL</code> configuration setting on a client system:</p> <ul> <li>Is a string value with a semicolon-separated (<code>;</code>) list of known Stratum 1 servers;</li> <li>Should be enclosed in quotes;</li> <li>Specifies each Stratum 1 as a URL that starts with <code>http://</code>, and ends with <code>/cvmfs/@fqrn@</code></li> </ul> <p>For example:</p> <pre><code>CVMFS_SERVER_URL=\"http://s1.test.eu/cvmfs/@fqrn@;http://s1.test.us/cvmfs/@fqrn@\"\n</code></pre> <p>The <code>@fqrn@</code> substring is replaced by CernVM-FS with the fully qualified repository name, like <code>software.eessi.io</code>.</p> <p><code>CVMFS_SERVER_URL</code> should be specified in the domain-specific configuration file in <code>/etc/cvmfs</code> that is relevant for the CernVM-FS repository we have replicated on our Stratum 1.</p> <p>For <code>software.eessi.io</code>, we should add the following to <code>/etc/cvmfs/domain.d/eessi.io.local</code>:</p> <pre><code>CVMFS_SERVER_URL=\"http://STRATUM1_IP/cvmfs/@fqrn@\"\n</code></pre> <p>in which \"<code>STRATUM1_IP</code>\" must be replaced with (you guessed it) the IP address or hostname of the private Stratum 1 replica server.</p> <p>To apply the configuration change, run <code>cvmfs_config reload</code>:</p> <pre><code>sudo cvmfs_config reload\n</code></pre>"},{"location":"access/stratum1/#testing","title":"Testing","text":"<p>To verify that the client configuration was changed correctly, use <code>cvmfs_config stat</code> (which requires that the repository is mounted):</p> <pre><code>ls /cvmfs/software.eessi.io\ncvmfs_config stat -v software.eessi.io\n</code></pre> <p>The output line that starts with <code>Connection</code> should mention <code>online</code>, like this:</p> <pre><code>Connection: http://.../cvmfs/software.eessi.io through proxy DIRECT (online)\n</code></pre> <p>The <code>proxy DIRECT</code> indicates that we are not using a proxy server yet in this setup.</p> <p>You can also use <code>curl</code> to check the connection to the Stratum 1, by letting it print the HTTP header for the <code>.cvmfspublished</code> file in the root of the repository:</p> <pre><code>curl --head http://STRATUM1_IP/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre> <p>the first line of the output should be something like: <pre><code>HTTP/1.1 200 OK\n</code></pre></p> <p>If instead you see <code>403 Forbidden</code> then the proxy server is blocking the connection: <pre><code>HTTP/1.1 403 Forbidden\n</code></pre></p>"},{"location":"access/stratum1/#proxy-private-stratum-1","title":"Proxy + private Stratum 1","text":"<p>To have a more complete view, let's now also bring the proxy server back in the game.</p>"},{"location":"access/stratum1/#reconfigure-squid-proxy","title":"Reconfigure Squid proxy","text":"<p>First we need to make a small but important change to the configuration of the Squid proxy, to ensure that the proxy server is allowed to connect to the private Stratum 1 replica server.</p> <p>Update the ACL for the Stratum 1 servers in <code>/etc/squid/squid.conf</code> on the proxy server by adding the IP address of the private Stratum 1: <pre><code># replace STRATUM1_IP with the IP address of the private Stratum 1\nacl stratum_ones dstdomain .eessi.science STRATUM1_IP\n</code></pre></p> <p>And then reload for configuration for the Squid proxy service:</p> <pre><code>sudo systemctl reload squid\n</code></pre>"},{"location":"access/stratum1/#client-configuration_1","title":"Client configuration","text":"<p>We also need to update the client configuration to restore the <code>CVMFS_HTTP_PROXY</code> line in <code>/etc/cvmfs/default.local</code>, like we did when using the proxy server:</p> <pre><code># replace PROXY_IP with the IP address of the proxy server\nCVMFS_HTTP_PROXY=\"http://PROXY_IP:3128\"\n</code></pre> <p>Don't forget to reload the CernVM-FS client configuration:</p> <pre><code>sudo cvmfs_config reload\n</code></pre>"},{"location":"access/stratum1/#testing_1","title":"Testing","text":"<p>To test whether the setup using both the proxy server and the Stratum 1 replica server works, we can try accessing the EESSI repository, for example by sourcing the initialisation script:</p> <pre><code>source /cvmfs/software.eessi.io/versions/2023.06/init/bash\n</code></pre> <p>The output of <code>sudo cvmfs_config stat -v software.eessi.io</code> should include a <code>Connection</code> line that ends with <code>(online)</code>, like this:</p> <pre><code>Connection: http://STRATUM1_IP/cvmfs/software.eessi.io through proxy http://PROXY_IP:3128 (online)\n</code></pre> <p>You can also use <code>curl</code> to check whether the Stratum 1 can be reached via the proxy server:</p> <pre><code>http_proxy=http://PROXY_IP:3128 curl --head http://STRATUM1_IP/cvmfs/software.eessi.io/.cvmfspublished\n</code></pre>"},{"location":"access/stratum1/#conclusions","title":"Conclusions","text":"<p>With a private Stratum 1 replica server, we have a more production-ready setup in place for using CernVM-FS.</p> <p>Using both a proxy server and a Stratum 1 replica server is another step in that direction, since it further improves the resilience, maintainability, scalability, and performance of the setup (since the proxy server can serve request from its memory cache).</p> <p>For the sake of demonstration we have used two separate systems for the Stratum 1 replica server and the proxy server, but both services can also be installed and configuration on the same server, and also installing multiple proxy servers is sensible to improve load balancing, for example to serve different HPC clusters that have significantly different workload mixes.</p> <p>(next: Alternative ways to access CernVM-FS repositories)</p>"},{"location":"appendix/terminology/","title":"CernVM-FS Terminology","text":"<p>An overview of terms used in the context of CernVM-FS, in alphabetical order.</p>"},{"location":"appendix/terminology/#catalog","title":"Catalog","text":"<p>A catalog of a CernVM-FS repository is a table that lists files and directories along with the corresponding metadata (sizes, timestamps, etc.).</p> <p>Catalogs can be nested: subtrees of the repository may have their own catalog.</p> <p>For more information on the catalog concept, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#cernvm","title":"CernVM","text":"<p>CernVM is a virtual machine image based on CentOS combined with a custom, virtualization-friendly Linux kernel, and which includes the CernVM-FS client software.</p> <p>It is used for the CERN Large Hadron Collider (LHC) experiment, and was created to remove a need for the installation of the experiment software and to minimize the number of platforms (compiler-OS combinations) on which experiment software needs to be supported and tested.</p> <p>While originally developed in conjunction, the CernVM File System today is a product that is completely independent from the CernVM virtual appliance.</p> <p>For more information on CernVM, see the website and documentation.</p>"},{"location":"appendix/terminology/#cvmfs","title":"CernVM-FS","text":"<p>(see What is CernVM-FS?)</p>"},{"location":"appendix/terminology/#client","title":"Client","text":"<p>A client in the context of CernVM-FS is a computer system on which a CernVM-FS repository is being accessed, on which it will be presented as a POSIX read-only file system in a subdirectory of <code>/cvmfs</code>.</p>"},{"location":"appendix/terminology/#proxy","title":"Proxy","text":"<p>A proxy, also referred to as squid proxy, is a forward caching proxy server which acts as an intermediary between a CernVM-FS client and the Stratum-1 replica servers.</p> <p>It is used to improve the latency observed when accessing the contents of a repository, and to reduce the load on the Stratum-1 replica servers.</p> <p>A commonly used proxy is Squid.</p> <p>For more information on proxies, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#publishing","title":"Publishing","text":"<p>Publishing is the process of adding more files to a CernVM-FS repository, which is done via a transaction mechanism, and is on possible on the Stratum-0 server, via a publisher, or via a repository gateway.</p> <p>The workflow of publishing content is covered in detail in the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#repository","title":"Repository","text":"<p>A CernVM-FS repository is where the files and directories that you want to distribute via CernVM-FS are stored, which usually correspond to a collection of software installations.</p> <p>It is a form of content-addressable storage (CAS), and is the single source of (new) data for the file system being presented as a subdirectory of <code>/cvmfs</code> on client systems that mount the repository.</p> <p>Note</p> <p>A CernVM-FS repository includes software installations, not software packages like RPMs.</p>"},{"location":"appendix/terminology/#software-installations","title":"Software installations","text":"<p>An important distinction for a CernVM-FS repository compared to the more traditional notion of a software repository is that a CernVM-FS repository provides access to the individual files that collectively form a particular software installation, as opposed to housing a set of software packages like RPMs, each of which being a collection of files for a particular software installation that are packed together in a single package to distribute as a whole.</p> <p>Note</p> <p>This is an important distinction, since CernVM-FS enables only downloading the specific files that are required to perform a particular task with a software installation, which often is a small subset of all files that are part of that software installation.</p>"},{"location":"appendix/terminology/#stratum0","title":"Stratum 0 server","text":"<p>A Stratum 0 server, often simply referred to a Stratum 0 (Stratum Zero), is the central server for one or more CernVM-FS repositories.</p> <p>It is the single source of (new) data, since it hosts the master copy of the repository contents.</p> <p>Adding or updating files in a CernVM-FS repository (publishing) can only be done on the Stratum 0 server, either directly via the <code>cvmfs_server publish</code> command, or indirectory via a publisher server.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"appendix/terminology/#stratum1","title":"Stratum 1 replica server","text":"<p>A Stratum 1 replica server, often simply referred to a Stratum 1 (Stratum One), is a standard web server that acts as a mirror server for one or more CernVM-FS repositories.</p> <p>It holds a complete copy of the data for each CernVM-FS repository it serves, and automatically synchronises with the Stratum 0.</p> <p>There is typically a network of several Stratum 1 servers for a CernVM-FS repository, which are geographically distributed.</p> <p>Clients can be configured to automatically connect to the closest Stratum 1 server by using the CernVM-FS GeoAPI.</p> <p>For more information, see the CernVM-FS documentation.</p>"},{"location":"cvmfs/","title":"Introduction to CernVM-FS","text":"<ul> <li>What is CernVM-FS?</li> <li>Technical details</li> <li>Flagship repositories</li> </ul>"},{"location":"cvmfs/flagship-repositories/","title":"Flagship CernVM-FS repositories","text":""},{"location":"cvmfs/flagship-repositories/#lhc-experiments","title":"LHC experiments","text":"<p>CernVM-FS repositories are used to distribute the software required to analyse the data produced by the Large Hadron Collider (LHC) at each of the LHC experiments.</p> <p>Examples include (click to browse repository contents):</p> <ul> <li><code>/cvmfs/alice.cern.ch</code>: software for ALICE experiment</li> <li><code>/cvmfs/atlas.cern.ch</code>: software for ATLAS experiment</li> <li><code>/cvmfs/cms.cern.ch</code>: software for CMS experiment</li> <li><code>/cvmfs/lhcb.cern.ch</code>: software for LHCb experiment</li> <li><code>/cvmfs/sft.cern.ch</code>: LCG Software Stacks</li> </ul>"},{"location":"cvmfs/flagship-repositories/#the-alliance","title":"The Alliance","text":"<p>The Digital Research Alliance of Canada, a.k.a. The Alliance and formerly known as Compute Canada, uses CernVM-FS to distribute the software stack for the Canadian national compute clusters.</p> <p>Documentation on using their CernVM-FS repository <code>/cvmfs/soft.computecanada.ca</code> is available here, and an overview of available software is available here.</p>"},{"location":"cvmfs/flagship-repositories/#unpacked-containers","title":"Unpacked containers","text":"<p>CernVM-FS repositories can be used to provide an efficient way to access container images, by serving unpacked container images that can be consumed by container runtimes such as Apptainer.</p> <p>Examples include:</p> <ul> <li><code>/cvmfs/unpacked.cern.ch</code></li> <li><code>/cvmfs/singularity.opensciencegrid.org</code></li> </ul> <p>More information on <code>unpacked.cern.ch</code> is available in the CernVM-FS documentation:</p> <ul> <li>Container Images and CernVM-FS</li> <li>Working with DUCC and Docker Images</li> </ul>"},{"location":"cvmfs/flagship-repositories/#eessi","title":"EESSI","text":"<p>The European Environment for Scientific Software Installations (EESSI) provides optimized installations of scientific software for <code>x86_64</code> (Intel + AMD) and <code>aarch64</code> (64-bit Arm) systems that work on any Linux distribution.</p> <p>We will use EESSI as an example CernVM-FS repository throughout this tutorial.</p> <p>(next: What is EESSI?)</p>"},{"location":"cvmfs/technical-details/","title":"Technical details of CernVM-FS","text":"<p>CernVM-FS is implemented as a POSIX read-only filesystem in user space (FUSE) with repositories of files that are served via outgoing HTTP connections only, thus avoiding problems with firewalls.</p> <p>Files in a CernVM-FS repository are automatically downloaded on-demand to a client system as they are accessed, from web servers that support the CernVM-FS repository being used.</p> <p>Internally, CernVM-FS uses content-adressable storage (CAS) and Merkle trees (like Git also does) to store file data and metadata.</p>"},{"location":"cvmfs/technical-details/#caching","title":"Caching","text":"<p>CernVM-FS uses a caching mechanism with a least-recently used (LRU) cache replacement policy, in which configurable local client cache is populated via either a forward proxy server (like Squid), or from a Stratum-1 replica server.</p> <p>Both the proxy and the replica server could be within the same local network as the client, or not.</p> <p>To help reduce performance problems regarding network latency and bandwidth, clients can leverage the Geo API supported by CernVM-FS Stratum-1 replica servers to automatically sort them geographically, in order to prioritize connecting to the closest ones.</p> <p>Furthermore, additional caches can be made available to CernVM-FS, such as an alien cache on a shared cluster filesystem like GPFS or Lustre that is not managed by CernVM-FS, and a Content Delivery Network (CDN) can be used to help limit the time required to download files that are not cached yet.</p> <p>(next: Flagship CernVM-FS repositories)</p>"},{"location":"cvmfs/what-is-cvmfs/","title":"What is CernVM-FS?","text":"<p>CernVM-FS, the CernVM File System (also known as CVMFS), is a file distribution service that is particularly well suited to distribute software installations across a large number of systems world-wide in an efficient way.</p> <p>From an end user perspective, files in a CernVM-FS repository are available read-only via a subdirectory in <code>/cvmfs</code>, with a user experience similar to that of an on-demand streaming service for music or video, but then (mainly) applied to software installations.</p>"},{"location":"cvmfs/what-is-cvmfs/#primary-use-case","title":"Primary use case","text":"<p>The primary use case of CernVM-FS is distributing software, and it provides several interesting features that support this, including:</p> <ul> <li>on-demand downloading and updating of repository contents;</li> <li>multi-level caching;</li> <li>de-duplication of files;</li> <li>compression of data;</li> <li>verification of data integrity;</li> </ul> <p>CernVM-FS has been proven to scale to billions of files and tens of thousands of clients.</p> <p>It was originally developed at CERN to let High Energy Physics (HEP) collaborations like the experiments at the Large Hadron Collider (LHC) deploy software on the Worldwide LHC Computing Grid (WLCG) infrastructure that is used to run data processing applications.</p> <p>The primary use case of distributing software is a particular one, since software often comprises many small files that are frequently opened and read as a whole, and frequent look-ups for files in multiple directories are triggered when search paths are examined.</p> <p>In certain cases, the CernVM-FS has also been used to distribute large data repositories.</p>"},{"location":"cvmfs/what-is-cvmfs/#features","title":"Features","text":""},{"location":"cvmfs/what-is-cvmfs/#features-ondemand","title":"On-demand downloading of files and metadata","text":"<p>The metadata and content of files included in a CernVM-FS repository are automatically downloaded on-demand as files and directories are being accessed, which is akin to streaming services for music, movies, and TV series.</p> <p>This happens fully transparently, as the contents of a repository are exposed by CernVM-FS as if it were a local (read-only) file system. Hence, clients that access a CernVM-FS repository typically do not actually have a local copy of all files included in that repository, but only have a limited set of files and metadata directly available: those which were most recently accessed.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-updating","title":"Automatic updates","text":"<p>CernVM-FS clients automatically pull in updates to the contents of a repository as they are published server-side. This happens in transactions, to ensure that clients observe a consistent state of the repository.</p> <p>Once a CernVM-FS repository is accessible on a client system, no subsequent actions must be taken to keep clients up-to-date other than updating CernVM-FS itself on a regular basis.</p> <p>This significantly limits the maintenance burden, since no action is required on client systems to update the software stack that is provided through a CernVM-FS repository, since the updates are streamed in automatically by CernVM-FS.</p> <p>Only the CernVM-FS client should be updated on a regular basis on client systems.</p> <p>For more elaborate setups that involve proxies or CernVM-FS replica (mirror) servers, additional maintenance is necessary, but again only to update the CernVM-FS components themselves.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-caching","title":"Multi-level caching","text":"<p>CernVM-FS uses a multi-level caching hierarchy to reduce the latency observed when accessing repository contents. Caching is an essential part of CernVM-FS, since the contents of a CernVM-FS repository are downloaded on-demand as they are accessed.</p> <p>The caching mechanism employed by CernVM-FS goes way beyond the standard (in-memory) Linux kernel file system cache, and consists of a local client cache, an optional forward proxy server that acts as an intermediary cache level, and a distributed network of mirror servers that support the CernVM-FS repository being accessed.</p> <p>When a part of the repository is being accessed that is not available yet in the local client cache, CernVM-FS will traverse the multi-level cache hierarchy to obtain the necessary data and update the local client cache with it, so the files being accessed can be served with low latency.</p> <p>Proxy and mirror servers scale horizontally: the CernVM-FS client makes automatic use of multiple deployed service instances for load-balancing and high-availability.</p> <p>We will explore this multi-level caching mechanism in more detail in this tutorial.</p> <p>See here more technical details on CernVM-FS caching.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-deduplication","title":"De-duplication of files","text":"<p>CernVM-FS stores the contents of a file only once, even when it is included multiple times in a particular repository at different paths.</p> <p>This can result in a significant reduction in storage capacity that is required to host a large software stack, especially when identical files are spread out across the repository, as often happens with particular files like example data files across multiple versions of the same software.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-compression","title":"Compression of data","text":"<p>CernVM-FS stores file content compressed on the server, which not only further reduces required storage space but also significantly limits the network bandwidth that is required to download (and serve) the contents of a repository.</p> <p>On the client side, the data is transparently decompressed when the files included in a CernVM-FS repository are presented under <code>/cvmfs</code> as a normal (read-only) file system.</p>"},{"location":"cvmfs/what-is-cvmfs/#features-data-integrity","title":"Verification of data integrity","text":"<p>The integrity of data provided by a CernVM-FS server is ensured on a client system by verifying a cryptographic hash, which is again a direct result of content-addressable storage mechanism that is used by CernVM-FS. This is an essential security aspect since CernVM-FS uses (possibly untrusted) caches and HTTP connections to distribute the contents of a repository.</p> <p>(next: Technical details of CernVM-FS)</p>"},{"location":"eessi/","title":"EESSI","text":""},{"location":"eessi/#european-environment-for-scientific-software-installations","title":"European Environment for Scientific Software Installations","text":"<ul> <li>What is EESSI?</li> <li>Motivation &amp; Goals</li> <li>Inspiration</li> <li>High-level design</li> <li>Using EESSI</li> </ul>"},{"location":"eessi/high-level-design/","title":"High-level design of EESSI","text":"<p>The design of EESSI is very similar to that of the Compute Canada software stack it is inspired by, and is aligned with the motivation and goals of the project.</p> <p>In the remainder of this section of the tutorial, we will explore the layered structure of the EESSI software stack, and how to use it.</p> <p>In the next section will cover in detail how you can get access to EESSI (and other publicly available CernVM-FS repositories).</p>"},{"location":"eessi/high-level-design/#layered-structure","title":"Layered structure","text":"<p>The EESSI project consists of 3 layers, which are constructed by leveraging various open source software projects.</p> <p> </p>"},{"location":"eessi/high-level-design/#filesystem_layer","title":"Filesystem layer","text":"<p>The filesystem layer is responsible for distributing the EESSI software stack to systems on which is it used.</p> <p>This is done using CernVM-FS, which is a mature open source software project that was created exactly for this purpose: to distribute software installations worldwide reliably and efficiently in a scalable way. As such, it aligns very well with the goals of EESSI.</p> <p>The CernVM-FS repository for EESSI is <code>/cvmfs/software.eessi.io</code>, which is part of the default CernVM-FS configuration since 21 November 2023, so no additional action is required to gain access to it other than installing and configuration the client component of CernVM-FS.</p> <p>More on that in the next section of this tutorial.</p> Note on the EESSI pilot repository (click to expand) <p>There is also a \"pilot\" CernVM-FS repository for EESSI (<code>/cvmfs/pilot.eessi-hpc.org</code>), which was primarily used to gain experience with CernVM-FS in the early years of the EESSI project.</p> <p>Although it is still available currently, we do not recommend using it.</p> <p>Not only will you need to install the CernVM-FS configuration for EESSI to gain access to it, there also are no guarantees that the EESSI pilot repository will remain stable or even available, nor that the software installations it provides are actually functional, since it may be used for experimentation purposes by the EESSI maintainers.</p>"},{"location":"eessi/high-level-design/#compat_layer","title":"Compatibility layer","text":"<p>The compatibility layer of EESSI levels the ground across different (versions of) the Linux operating system (OS) of client systems that use the software installations provided by EESSI.</p> <p>It consists of a limited set of libraries and tools that are installed in a non-standard filesystem location (a \"prefix\"), which were built from source for the supported CPU families using Gentoo Prefix.</p> <p>The installation path of the EESSI compatibility layer corresponds to the <code>compat</code> subdirectory of a specific version of EESSI (like <code>2023.06</code>) in the EESSI CernVM-FS repository, which is specific to a particular type of OS (currently only <code>linux</code>) and CPU family (currently <code>x86_64</code> and <code>aarch64</code>):</p> <pre><code>$ ls /cvmfs/software.eessi.io/versions/2023.06/compat\nlinux\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux\naarch64  x86_64\n\n$ ls /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64\nbin  etc  lib  lib64  opt  reprod  run  sbin  stage1.log  stage2.log  stage3.log  startprefix  tmp  usr  var\n\n$ ls -l /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64\ntotal 4923\n-rwxr-xr-x 1 cvmfs cvmfs  210528 Nov 15 11:22 ld-linux-x86-64.so.2\n...\n-rwxr-xr-x 1 cvmfs cvmfs 1876824 Nov 15 11:22 libc.so.6\n...\n-rwxr-xr-x 1 cvmfs cvmfs  911600 Nov 15 11:22 libm.so.6\n...\n</code></pre> <p>Libraries included in the compatibility layer can be used on any Linux client system, as long as the CPU family is compatible and taken into account.</p> <pre><code>$ uname -m\nx86_64\n\n$ cat /etc/redhat-release\nRed Hat Enterprise Linux release 8.8 (Ootpa)\n\n$ /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib64/libc.so.6\nGNU C Library (Gentoo 2.37-r7 (patchset 10)) stable release version 2.37.\n...\n</code></pre> <p>By making sure that the software installations included in EESSI only rely on tools and libraries provided by the compatibility layer, and do not (directly) require anything from the client OS, we can ensure that they can be used in a broad variety of Linux systems, regardless of the (version of) Linux distribution being used.</p> <p>Note</p> <p>This is very similar to the OS tools and libraries that are included in container images, except that no container runtime is involved here (typically), only CernVM-FS.</p>"},{"location":"eessi/high-level-design/#software_layer","title":"Software layer","text":"<p>The top layer of EESSI is called the software layer, which contains the actual scientific software applications and their dependencies.</p>"},{"location":"eessi/high-level-design/#easybuild","title":"EasyBuild to install software","text":"<p>Building, managing, and optimising the software installations included in the software layer is layer is done using EasyBuild, a well-established software build and installation framework for managing (scientific) software stacks on High-Performance Computing (HPC) systems.</p>"},{"location":"eessi/high-level-design/#lmod","title":"Lmod as user interface","text":"<p>Next to installing the software itself, EasyBuild also automatically generates environment module files. These files, which are essentially small Lua scripts, are consumed via Lmod, a modern implementation of the concept of environment modules which provides a user-friendly interface to end users of EESSI.</p>"},{"location":"eessi/high-level-design/#cpu_detection","title":"CPU detection via <code>archspec</code> or <code>archdetect</code>","text":"<p>The initialisation script that is included in the EESSI repository automatically detects the CPU family and microarchitecture of a client system by leveraging either <code>archspec</code>, a small Python library, or <code>archdetect</code>, a minimal pure bash implementation of the same concept.</p> <p>Based on the features of the detected CPU microarchitecture, the EESSI initialisation script will automatically select the best suited subdirectory of the software layer that contains software installations that are optimised for that particular type of CPU, and update the session environment to start using it.</p>"},{"location":"eessi/high-level-design/#software_layer_structure","title":"Structure of the software layer","text":"<p>For now, we just briefly show the structure of <code>software</code> subdirectory that contains the software layer of a particular version of EESSI below.</p> <p>The <code>software</code> subdirectory is located at the same level as the <code>compat</code> directory for a particular version of EESSI, along with the <code>init</code> subdirectory that provides initialisation scripts:</p> <pre><code>$ cd /cvmfs/software.eessi.io/versions/2023.06\n$ ls\ncompat  init  software\n</code></pre> <p>In the <code>software</code> subdirectory, a subtree of directories is located that contains software installations that are specific to a particular OS family (only <code>linux</code> currently) and a specific CPU microarchitecture (with <code>generic</code> as a fallback):</p> <pre><code>$ ls software\nlinux\n\n$ ls software/linux\naarch64  x86_64\n\n$ ls software/linux/aarch64\ngeneric  neoverse_n1  neoverse_v1\n\n$ ls software/linux/x86_64\namd  generic  intel\n\n$ ls software/linux/x86_64/amd\nzen2  zen3\n\n$ ls software/linux/x86_64/intel\nhaswell  skylake_avx512\n</code></pre> <p>Each subdirectory that is specific to a particular CPU microarchitecure provides the actual optimised software installations (in <code>software</code>) and environment module files (in <code>modules/all</code>).</p> <p>Here we explore the path that is specific to AMD Milan CPUs, which have the Zen3 microarchitecture, focusing on the installations of OpenBLAS:</p> <pre><code>$ ls software/linux/x86_64/amd/zen3\nmodules  software\n\n$ ls software/linux/x86_64/amd/zen3/software\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/\n0.3.21-GCC-12.2.0  0.3.23-GCC-12.3.0\n\n$ ls software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/\nbin  easybuild  include  lib  lib64\n\n$ ls software/linux/x86_64/amd/zen3/modules/all\n\n... (long list of directories of software names omitted) ...\n\n$ ls software/linux/x86_64/amd/zen3/modules/all/OpenBLAS\n0.3.21-GCC-12.2.0.lua  0.3.23-GCC-12.3.0.lua\n</code></pre> <p>Each of the other subdirectories for specific CPU microarchitectures will have the exact same structure, and provide the same software installations and accompanying environment module files to access them with Lmod.</p> <p>A key aspect here is that binaries and libraries that make part of the software installations included in the EESSI software layer only rely on libraries provided by the compatibility layer and/or other software installations in the EESSI software layer.</p> <p>See for example libraries to which the OpenBLAS library links:</p> <pre><code>$ ldd software/linux/x86_64/amd/zen3/software/OpenBLAS/0.3.23-GCC-12.3.0/lib/libopenblas.so\n    linux-vdso.so.1 (0x00007ffd4373d000)\n    libm.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libm.so.6 (0x000014d0884c8000)\n    libgfortran.so.5 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgfortran.so.5 (0x000014d087115000)\n    libgomp.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgomp.so.1 (0x000014d088480000)\n    libc.so.6 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libc.so.6 (0x000014d086f43000)\n    /lib64/ld-linux-x86-64.so.2 (0x000014d08837e000)\n    libpthread.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libpthread.so.0 (0x000014d088479000)\n    libdl.so.2 =&gt; /cvmfs/software.eessi.io/versions/2023.06/compat/linux/x86_64/lib/../lib64/libdl.so.2 (0x000014d088474000)\n    libquadmath.so.0 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libquadmath.so.0 (0x000014d08842d000)\n    libgcc_s.so.1 =&gt; /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen3/software/GCCcore/12.3.0/lib64/libgcc_s.so.1 (0x000014d08840d000)\n</code></pre> Note on <code>/lib64/ld-linux-x86-64.so.2</code> (click to expand) <p>The <code>/lib64/ld-linux-x86-64.so.2</code> path, which corresponds to the dynamic linker/loader of the Linux client OS, that is shown in the output of <code>ldd</code> above is a bit misleading.</p> <p>It only pops up because we are running the <code>ldd</code> command provided by the client OS, which typically resides at <code>/usr/bin/ldd</code>.</p> <p>When actually running software provided by the EESSI software layer, the loader provided by the EESSI compatibility layer is used to launch binaries.</p> <p>We will explore the EESSI software layer a bit more in the next subsection of this tutorial, when we demonstrate how to use the software installations provided in the EESSI software layer.</p> <p>(next: Using EESSI)</p>"},{"location":"eessi/inspiration/","title":"Inspiration for EESSI","text":"<p>The EESSI concept is heavily inspired by software stack provided by the Digital Research Alliance of Canada (a.k.a. The Alliance, formerly known as Compute Canada), which is a shared software stack used on all national host sites for Advanced Research Computing in Canada that is distributed across Canada (and beyond) using CernVM-FS; see also here.</p> <p>EESSI is significantly more ambitious in its goals however, in various ways.</p> <p>It intends to support a broader range of system architectures than what is currently supported by the Compute Canada software stack, like Arm 64-bit microprocessors, accelerators beyond NVIDIA GPUs, etc.</p> <p>In addition, EESSI is set up to be a community project, by setting up services and infrastructure to automate the software build and installation process as much as possible, providing extensive documentation and support to end users, user support teams, and system administrators who want to employ EESSI, and allowing contributors to propose additions to the software stack.</p> <p>The design of the Compute Canada software stack is discussed in detail in the PEARC'19 paper \"Providing a Unified Software Environment for Canada\u2019s National Advanced Computing Centers\".</p> <p>It has also been presented at the 5th EasyBuild User Meeting, see slides and talk recording.</p> <p>More information on the Compute Canada software stack is available in their documentation, and in their overview of available software.</p> <p>(next: High-level Overview of EESSI)</p>"},{"location":"eessi/motivation-goals/","title":"Motivation &amp; Goals of EESSI","text":""},{"location":"eessi/motivation-goals/#motivation","title":"Motivation","text":"<p>EESSI is motivated by the observation that the landscape of computational science is changing in various ways, including:</p> <ul> <li>Increasing diversity in system architectures: additional families of general-purpose   microprocessors including Arm 64-bit (<code>aarch64</code>) and   RISC-V on top of the well-established Intel and AMD processors (both <code>x86_64</code>),   and different types of GPUS (NVIDIA, AMD, Intel);</li> <li>Rapid expansion of computational science beyond traditional domains like physics and computational chemistry,   including bioinformatis, Machine Learning (ML) and Artificial Intelligence (AI), etc.,   which leads to a significant growth of the software stack that is used for running scientific workloads;</li> <li>Emergence of commercial cloud infrastructure (Amazon EC2,   Microsoft Azure, ...)   that has competitive advantages over on-premise infrastructure for computational workloads, such as near-instant   availability, increased flexibility, a broader variety of hardware platforms, and faster access to   new generations of microprocessors;</li> <li>Limited manpower that is available in the HPC user support teams that are responsible for helping   scientists with running the software they require on high-end (and complex) infrastructure like supercomputers   (and beyond);</li> </ul> <p>Collectively, these indicate that there is a strong need for more collaboration on building and installing scientific software to avoid duplicate work across computational scientists and HPC user support teams.</p>"},{"location":"eessi/motivation-goals/#goals","title":"Goals","text":"<p>The main goal of EESSI is to provide a collection of scientific software installations that work across a wide range of different platforms, including HPC clusters, cloud infrastructure, and personal workstations and laptops, without making compromises on the performance of that software.</p> <p>While initially the focus of EESSI is to support Linux systems with established system architectures like AMD + Intel CPUs and NVIDIA GPUs, the ambition is to also cover emerging technologies like Arm 64-bit CPUs, other accelerators like the AMD Instinct and Intel Xe, and eventually also RISC-V microprocessors.</p> <p>The software installations included in EESSI are optimized for specific generations of microprocessors by targeting a variety of instruction set architectures (ISAs), like for example Intel and AMD processors supporting the AVX2 or AVX-512 instructions, and Arm processors that support SVE instructions.</p> <p>(next: Inspiration for EESSI)</p>"},{"location":"eessi/support/","title":"Getting support for EESSI","text":"<p>Thanks to the funding provided by the MultiXscale EuroHPC JU Centre-of-Excellence, a dedicated support team is available to provide help on accessing or using EESSI.</p> <p>If you have any questions, or if you are experiencing problems, do not hesitate to reach out by either opening an issue in the EESSI support portal, or sending an email to <code>support@eessi.io</code>.</p> <p>For more information, see the support section of the EESSI documentation.</p> <p>(next: CernVM-FS client system)</p>"},{"location":"eessi/using-eessi/","title":"Using EESSI","text":"<p>Using the software installations provided by the EESSI CernVM-FS repository <code>software.eessi.io</code> is fairly straightforward.</p> <p>Let's break it down step by step.</p>"},{"location":"eessi/using-eessi/#0-is-eessi-available","title":"0) Is EESSI available?","text":"<p>First, check whether the EESSI CernVM-FS repository is available on your system.</p> <p>Try checking the contents of the <code>/cvmfs/software.eessi.io</code> directory with the <code>ls</code> command:</p> <pre><code>$ ls /cvmfs/software.eessi.io\nREADME.eessi  host_injections  versions\n</code></pre> <p>If you see an error message like \"<code>No such file or directory</code>\", then either the CernVM-FS client is not installed on your system, or the configuration for the EESSI repository is not available. In that case, you may want to revisit the Accessing a CernVM-FS repository section, or go through the Troubleshooting section.</p> Don't be fooled by <code>autofs</code> (click to expand) <p>The <code>/cvmfs</code> directory may seem empty at first, because CernVM-FS repositories are automatically mounted as they are accessed via <code>autofs</code>.</p> <p>So rather than just using \"<code>ls /cvmfs/</code>\" to check which CernVM-FS repositories are available on your system, you should try to directly access a specific repository as shown above for EESSI with <code>ls /cvmfs/software.eessi.io</code> .</p> <p>For more information on various aspects of mounting of CernVM-FS repositories, see the CernVM-FS documentation.</p>"},{"location":"eessi/using-eessi/#init","title":"1) Initialise shell environment","text":"<p>If the EESSI repository is available, you can proceed to preparing your shell environment for using a particular version of EESSI by sourcing the provided initialisation script by running the <code>source</code> command:</p> <pre><code>$ source /cvmfs/software.eessi.io/versions/2023.06/init/bash\nFound EESSI repo @ /cvmfs/software.eessi.io/versions/2023.06!\narchdetect says x86_64/amd/zen2\nUsing x86_64/amd/zen2 as software subdirectory.\nUsing /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all as the directory to be added to MODULEPATH.\nFound Lmod configuration file at /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/.lmod/lmodrc.lua\nInitializing Lmod...\nPrepending /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all to $MODULEPATH...\nEnvironment set up to use EESSI (2023.06), have fun!\n</code></pre> Details on changes made to the shell environment (click to expand) <p>The initialisation script is a simple bash script that changes a couple of environment variables:</p> <ul> <li>A set of <code>$EESSI_*</code> environment variables is defined;</li> <li>The <code>$PS1</code> environment variable that specifies the shell prompt   is updated to indicate that your shell session has been initialised for EESSI;</li> <li>The location of the tools provided by the EESSI compatibility layer are prepended to the <code>$PATH</code> environment variable;</li> <li>Lmod, which is included in the EESSI compatibility layer, is initialised to ensure that the <code>module</code> command is defined,   and that the Lmod spider cache that is included in the EESSI software layer is picked up;</li> <li>The location to the software installations that are optimised for the CPU microarchitecture of the client system   is prepended to the <code>$MODULEPATH</code> environment variable by running a \"<code>module use</code>\" command.</li> </ul> <p>Note how the CPU microarchitecture is being auto-detected, which determines which path that points to a set of environment module files is used to update <code>$MODULEPATH</code>.</p> <p>This ensures that the modules that will be loaded provide access to software installations from the EESSI software layer that are optimised for the system you are using EESSI on.</p>"},{"location":"eessi/using-eessi/#2-load-modules","title":"2) Load module(s)","text":"<p>After initialising your shell environment for using EESSI, you can start exploring the EESSI software layer using the <code>module</code> command.</p> <p>Using <code>module avail</code> (or <code>ml av</code>), you can check which software is available. Without extra arguments, <code>module avail</code> will produce an overview of all available software. By passing an extra argument you can filter the results and search for specific software:</p> <pre><code>$ module avail tensorflow\n\n----- /cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/modules/all -----\n\n    TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>To start using software you should load the corresponding environment module files using <code>module load</code> (or <code>ml</code>). For example:</p> <pre><code>$ module load TensorFlow/2.13.0-foss-2023a\n</code></pre> <p>A <code>module load</code> command usually does not produce any output, but it updates your shell environment to make the software ready to use.</p> <p>For more information on the <code>module</code> command, see the User Guide for Lmod.</p>"},{"location":"eessi/using-eessi/#3-use-software","title":"3) Use software","text":"<p>After loading a module, you should be able to use the corresponding software.</p> <p>For example, after loading the <code>TensorFlow/2.13.0-foss-2023a</code> module, you can start a Python session and play with the <code>tensorflow</code> Python package:</p> <pre><code>$ python\n&gt;&gt;&gt; import tensorflow as tf\n&gt;&gt;&gt; tf.__version__\n'2.13.0'\n</code></pre> <p>Keep in mind that you are using a Python installation provided by the EESSI software layer here, not the Python version that may be provided by your client OS:</p> <pre><code>$ command -v python\n/cvmfs/software.eessi.io/versions/2023.06/software/linux/x86_64/amd/zen2/software/Python/3.11.3-GCCcore-12.3.0/bin/python\n</code></pre> Initial start-up delay (click to expand) <p>You may notice a bit of \"lag\" initially when starting to use software provided by the EESSI software layer.</p> <p>This is expected, since CernVM-FS may need to first download the files that are required to run the software you are using; see also On-demand downloading of files and metadata.</p> <p>You should not observe any significant start-up delays anymore when running the same software shortly after, since then CernVM-FS will be able to serve the necessary files from the local client cache; see also Multi-level caching.</p> <p>(next: Getting support for EESSI)</p>"},{"location":"eessi/what-is-eessi/","title":"What is EESSI?","text":"<p>The European Environment for Scientific Software Installations (EESSI, pronounced as \"easy\") is a collaboration between different European partners in the HPC (High Performance Computing) community.</p> <p>EESSI provides a common stack of optimized scientific software installations that work on any Linux distribution, and currently supports both <code>x86_64</code> (AMD/Intel) and <code>aarch64</code> (Arm 64-bit) systems, which is distributed via CernVM-FS.</p> <p>(next: Motivation &amp; Goals of EESSI)</p>"}]}